Yim, Ww., Fu, Y., Ben Abacha, A. et al. Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation. Sci Data 10, 586 (2023). https://doi.org/10.1038/s41597-023-02487-3

@misc{lage2019evaluationhumaninterpretabilityexplanation,
      title={An Evaluation of the Human-Interpretability of Explanation}, 
      author={Isaac Lage and Emily Chen and Jeffrey He and Menaka Narayanan and Been Kim and Sam Gershman and Finale Doshi-Velez},
      year={2019},
      eprint={1902.00006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00006}, 
}

Sutton, R.T., Pincock, D., Baumgart, D.C. et al. An overview of clinical decision support systems: benefits, risks, and strategies for success. npj Digit. Med. 3, 17 (2020). https://doi.org/10.1038/s41746-020-0221-y

@inproceedings{zhang-etal-2024-annotate,
    title = "Annotate the Way You Think: An Incremental Note Generation Framework for the Summarization of Medical Conversations",
    author = "Zhang, Longxiang  and
      Hart, Caleb D.  and
      Burger, Susanne  and
      Schaaf, Thomas",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.105/",
    pages = "1173--1186",
    abstract = "The scarcity of public datasets for the summarization of medical conversations has been a limiting factor for advancing NLP research in the healthcare domain, and the structure of the existing data is largely limited to the simple format of conversation-summary pairs. We therefore propose a novel Incremental Note Generation (ING) annotation framework capable of greatly enriching summarization datasets in the healthcare domain and beyond. Our framework is designed to capture the human summarization process via an annotation task by instructing the annotators to first incrementally create a draft note as they accumulate information through a conversation transcript (Generation) and then polish the draft note into a reference note (Rewriting). The annotation results include both the reference note and a comprehensive editing history of the draft note in tabular format. Our pilot study on the task of SOAP note generation showed reasonable consistency between four expert annotators, established a solid baseline for quantitative targets of inter-rater agreement, and demonstrated the ING framework as an improvement over the traditional annotation process for future modeling of summarization."
}

@misc{hong2024argmedagentsexplainableclinicaldecision,
      title={ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Disscusion via Argumentation Schemes}, 
      author={Shengxin Hong and Liang Xiao and Xin Zhang and Jianxia Chen},
      year={2024},
      eprint={2403.06294},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2403.06294}, 
}

Ben-Zion, Z., Witte, K., Jagadish, A.K. et al. Assessing and alleviating state anxiety in large language models. npj Digit. Med. 8, 132 (2025). https://doi.org/10.1038/s41746-025-01512-6

Van Veen D, Van Uden C, Blankemeier L, Delbrouck JB, Aali A, Bluethgen C, Pareek A, Polacin M, Reis EP, Seehofnerová A, Rohatgi N, Hosamani P, Collins W, Ahuja N, Langlotz CP, Hom J, Gatidis S, Pauly J, Chaudhari AS. Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. Res Sq [Preprint]. 2023 Oct 30:rs.3.rs-3483777. doi: 10.21203/rs.3.rs-3483777/v1. Update in: Nat Med. 2024 Apr;30(4):1134-1142. doi: 10.1038/s41591-024-02855-5. PMID: 37961377; PMCID: PMC10635391.

@misc{piya2025contextualimprovingclinicaltext,
      title={ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs}, 
      author={Fahmida Liza Piya and Rahmatollah Beheshti},
      year={2025},
      eprint={2504.16394},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2504.16394}, 
}

Miyachi, Y., Ishii, O. & Torigoe, K. Design, implementation, and evaluation of the computer-aided clinical decision support system based on learning-to-rank: collaboration between physicians and machine learning in the differential diagnosis process. BMC Med Inform Decis Mak 23, 26 (2023). https://doi.org/10.1186/s12911-023-02123-5


Bayor A, Li J, Yang I, Varnfield M
Designing Clinical Decision Support Systems (CDSS)—A User-Centered Lens of the Design Characteristics, Challenges, and Implications: Systematic Review
J Med Internet Res 2025;27:e63733
URL: https://www.jmir.org/2025/1/e63733
DOI: 10.2196/63733

@ARTICLE{10.3389/frai.2025.1604034,
    
AUTHOR={Bailly, Alexandre  and Saubin, Antoine  and Kocevar, Gabriel  and Bodin, Jonathan },
           
TITLE={Divide and summarize: improve SLM text summarization},
          
JOURNAL={Frontiers in Artificial Intelligence},
          
VOLUME={Volume 8 - 2025},
  
YEAR={2025},
  
URL={https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1604034},
  
DOI={10.3389/frai.2025.1604034},
  
ISSN={2624-8212},
  
ABSTRACT={IntroductionText summarization is a longstanding challenge in natural language processing, with recent advancements driven by the adoption of Large Language Models (LLMs) and Small Language Models (SLMs). Despite these developments, issues such as the “Lost in the Middle” problem—where LLMs tend to overlook information in the middle of lengthy prompts—persist. Traditional summarization, often termed the “Stuff” method, processes an entire text in a single pass. In contrast, the “Map” method divides the text into segments, summarizes each independently, and then synthesizes these partial summaries into a final output, potentially mitigating the “Lost in the Middle” issue. This study investigates whether the Map method outperforms the Stuff method for texts that fit within the context window of SLMs and assesses its effectiveness in addressing the “Lost in the Middle” problem.MethodsWe conducted a two-part investigation: first, a simulation study using generated texts, paired with an automated fact-retrieval evaluation to eliminate the need for human assessment; second, a practical study summarizing scientific papers.ResultsResults from both studies demonstrate that the Map method produces summaries that are at least as accurate as those from the Stuff method. Notably, the Map method excels at retaining key facts from the beginning and middle of texts, unlike the Stuff method, suggesting its superiority for SLM-based summarization of smaller texts. Additionally, SLMs using the Map method achieved performance comparable to LLMs using the Stuff method, highlighting its practical utility.DiscussionBoth theoretical and practical studies suggest that using Map method for summarization with SLM allowed to address the “Lost in the Middle” problem and outperform Stuff method.}}


Easing the cognitive load of general practitioners: AI design principles for future-ready healthcare (RePEc:eee:techno:v:142:y:2025:i:c:s0166497225000409)
by Hor, Timothy (Shoon Chan) & Fong, Lee & Wynne, Katie & Verhoeven, Bert
<https://ideas.repec.org/a/eee/techno/v142y2025ics0166497225000409.html>

@misc{ehtesham2025enhancingclinicaldecisionsupport,
      title={Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework}, 
      author={Abul Ehtesham and Aditi Singh and Saket Kumar},
      year={2025},
      eprint={2506.13800},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2506.13800}, 
}

@misc{chen2025enhancingclinicaldecisionmakingintegrating,
      title={Enhancing Clinical Decision-Making: Integrating Multi-Agent Systems with Ethical AI Governance}, 
      author={Ying-Jung Chen and Ahmad Albarqawi and Chi-Sheng Chen},
      year={2025},
      eprint={2504.03699},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.03699}, 
}

Chen, X., Yi, H., You, M. et al. Enhancing diagnostic capability with multi-agents conversational large language models. npj Digit. Med. 8, 159 (2025). https://doi.org/10.1038/s41746-025-01550-0

Marcilly, R., Ammenwerth, E., Roehrer, E. et al. Evidence-based usability design principles for medication alerting systems. BMC Med Inform Decis Mak 18, 69 (2018). https://doi.org/10.1186/s12911-018-0615-9


Fraile Navarro, D., Coiera, E., Hambly, T.W. et al. Expert evaluation of large language models for clinical dialogue summarization. Sci Rep 15, 1195 (2025). https://doi.org/10.1038/s41598-024-84850-x


@Article{healthcare13172154,
AUTHOR = {Abbas, Qaiser and Jeong, Woonyoung and Lee, Seung Won},
TITLE = {Explainable AI in Clinical Decision Support Systems: A Meta-Analysis of Methods, Applications, and Usability Challenges},
JOURNAL = {Healthcare},
VOLUME = {13},
YEAR = {2025},
NUMBER = {17},
ARTICLE-NUMBER = {2154},
URL = {https://www.mdpi.com/2227-9032/13/17/2154},
PubMedID = {40941506},
ISSN = {2227-9032},
ABSTRACT = {Background: Theintegration of artificial intelligence (AI) into clinical decision support systems (CDSSs) has significantly enhanced diagnostic precision, risk stratification, and treatment planning. AI models remain a barrier to clinical adoption, emphasizing the critical role of explainable AI (XAI). Methods: This systematic meta-analysis synthesizes findings from 62 peer-reviewed studies published between 2018 and 2025, examining the use of XAI methods within CDSSs across various clinical domains, including radiology, oncology, neurology, and critical care. Model-agnostic techniques such as visualization models like Gradient-weighted Class Activation Mapping (Grad-CAM) and attention mechanisms dominated in imaging and sequential data tasks. Results: However, there are still gaps in user-friendly evaluation, methodological transparency, and ethical issues, as seen by the absence of research that evaluated explanation fidelity, clinician trust, or usability in real-world settings. In order to enable responsible AI implementation in healthcare, our analysis emphasizes the necessity of longitudinal clinical validation, participatory system design, and uniform interpretability measures. Conclusions: This review offers a thorough analysis of the state of XAI practices in CDSSs today, identifies methodological and practical issues, and suggests a path forward for AI solutions that are open, moral, and clinically relevant.},
DOI = {10.3390/healthcare13172154}
}





@article{Silva21042023,
author = {Andrew Silva and Mariah Schrum and Erin Hedlund-Botti and Nakul Gopalan and Matthew Gombolay},
title = {Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction},
journal = {International Journal of Human–Computer Interaction},
volume = {39},
number = {7},
pages = {1390--1404},
year = {2023},
publisher = {Taylor \& Francis},
doi = {10.1080/10447318.2022.2101698},


URL = { 
    
        https://doi.org/10.1080/10447318.2022.2101698
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10447318.2022.2101698
    
    

}

}


@inproceedings{krishna-etal-2021-generating,
    title = "Generating {SOAP} Notes from Doctor-Patient Conversations Using Modular Summarization Techniques",
    author = "Krishna, Kundan  and
      Khosla, Sopan  and
      Bigham, Jeffrey  and
      Lipton, Zachary C.",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.384/",
    doi = "10.18653/v1/2021.acl-long.384",
    pages = "4958--4972",
    abstract = "Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes. While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout. In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients. After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster. Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators. For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset. Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora."
}


@article{SCHOONDERWOERD2021102684,
title = {Human-centered XAI: Developing design patterns for explanations of clinical decision support systems},
journal = {International Journal of Human-Computer Studies},
volume = {154},
pages = {102684},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2021.102684},
url = {https://www.sciencedirect.com/science/article/pii/S1071581921001026},
author = {Tjeerd A.J. Schoonderwoerd and Wiard Jorritsma and Mark A. Neerincx and Karel {van den Bosch}},
keywords = {Explainable AI, Explainability, Causability, Human-centered design, Clinical decision making, Decision-support system, User study, Design patterns},
abstract = {Much of the research on eXplainable Artificial Intelligence (XAI) has centered on providing transparency of machine learning models. More recently, the focus on human-centered approaches to XAI has increased. Yet, there is a lack of practical methods and examples on the integration of human factors into the development processes of AI-generated explanations that humans prove to uptake for better performance. This paper presents a case study of an application of a human-centered design approach for AI-generated explanations. The approach consists of three components: Domain analysis to define the concept & context of explanations, Requirements elicitation & assessment to derive the use cases & explanation requirements, and the consequential Multi-modal interaction design & evaluation to create a library of design patterns for explanations. In a case study, we adopt the DoReMi-approach to design explanations for a Clinical Decision Support System (CDSS) for child health. In the requirements elicitation & assessment, a user study with experienced paediatricians uncovered what explanations the CDSS should provide. In the interaction design & evaluation, a second user study tested the consequential interaction design patterns. This case study provided a first set of user requirements and design patterns for an explainable decision support system in medical diagnosis, showing how to involve expert end users in the development process and how to develop, more or less, generic solutions for general design problems in XAI.}
}


@misc{chen2022humanintheloopabstractivedialoguesummarization,
      title={Human-in-the-loop Abstractive Dialogue Summarization}, 
      author={Jiaao Chen and Mohan Dodda and Diyi Yang},
      year={2022},
      eprint={2212.09750},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09750}, 
}

@inproceedings{wu-etal-2025-incremental,
    title = "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback",
    author = "Wu, Yisha  and
      Zhao, Cen  and
      Cao, Yuanpei  and
      Xu, Xiaoqing  and
      Mehdad, Yashar  and
      Ji, Mindy  and
      Cheng, Claire Na",
    editor = "Potdar, Saloni  and
      Rojas-Barahona, Lina  and
      Montella, Sebastien",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2025",
    address = "Suzhou (China)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-industry.140/",
    doi = "10.18653/v1/2025.emnlp-industry.140",
    pages = "2000--2015",
    ISBN = "979-8-89176-333-3",
    abstract = "We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' cognitive load and redundant review. Our approach combines a fine-tuned Mixtral-8{\texttimes}7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3{\%} reduction in case handling time compared to bulk summarization (with reductions of up to 9{\%} in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale."
}

@misc{sanwal2025layeredchainofthoughtpromptingmultiagent,
      title={Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models}, 
      author={Manish Sanwal},
      year={2025},
      eprint={2501.18645},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.18645}, 
}

@article{VILONE202189,
title = {Notions of explainability and evaluation approaches for explainable artificial intelligence},
journal = {Information Fusion},
volume = {76},
pages = {89-106},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001093},
author = {Giulia Vilone and Luca Longo},
keywords = {Explainable artificial intelligence, Notions of explainability, Evaluation methods},
abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models that lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested, coupled with several studies attempting to define the concept of explainability and its evaluation. This systematic review contributes to the body of knowledge by clustering all the scientific studies via a hierarchical system that classifies theories and notions related to the concept of explainability and the evaluation approaches for XAI methods. The structure of this hierarchy builds on top of an exhaustive analysis of existing taxonomies and peer-reviewed scientific material. Findings suggest that scholars have identified numerous notions and requirements that an explanation should meet in order to be easily understandable by end-users and to provide actionable information that can inform decision making. They have also suggested various approaches to assess to what degree machine-generated explanations meet these demands. Overall, these approaches can be clustered into human-centred evaluations and evaluations with more objective metrics. However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps and limitations, and it defines future research directions with explainability as the starting component of any artificial intelligent system.}
}


@article{article,
author = {Peffers, Ken and Tuunanen, Tuure and Rothenberger, Marcus and Chatterjee, S.},
year = {2007},
month = {01},
pages = {45-77},
title = {A design science research methodology for information systems research},
volume = {24},
journal = {Journal of Management Information Systems}
}

@article{article,
author = {Salimiparsa, Mozhgan and Lizotte, Daniel and Sedig, Kamran},
year = {2021},
month = {06},
pages = {},
title = {A User-Centered Design of Explainable AI for Clinical Decision Support},
journal = {Proceedings of the Canadian Conference on Artificial Intelligence},
doi = {10.21428/594757db.62860442}
}

@article{article,
author = {Harrison, Michael and Dullabh, Prashila},
year = {2022},
month = {05},
pages = {},
title = {Challenges and Opportunities for Advancing Patient-Centered Clinical Decision Support: Findings from a Horizon Scan},
volume = {29},
journal = {Journal of the American Medical Informatics Association},
doi = {10.1093/jamia/ocac059}
}

@misc{schneider2025policiesevaluationonlinemeeting,
      title={Policies and Evaluation for Online Meeting Summarization}, 
      author={Felix Schneider and Marco Turchi and Alex Waibel},
      year={2025},
      eprint={2502.03111},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.03111}, 
}

@misc{leduc2025realtimespeechsummarizationmedical,
      title={Real-time Speech Summarization for Medical Conversations}, 
      author={Khai Le-Duc and Khai-Nguyen Nguyen and Long Vo-Dang and Truong-Son Hy},
      year={2025},
      eprint={2406.15888},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.15888}, 
}

@misc{chen2025enhancingclinicaldecisionmakingintegrating,
      title={Enhancing Clinical Decision-Making: Integrating Multi-Agent Systems with Ethical AI Governance}, 
      author={Ying-Jung Chen and Ahmad Albarqawi and Chi-Sheng Chen},
      year={2025},
      eprint={2504.03699},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2504.03699}, 
}

@article{HAIG2006167,
title = {SBAR: A Shared Mental Model for Improving Communication Between Clinicians},
journal = {The Joint Commission Journal on Quality and Patient Safety},
volume = {32},
number = {3},
pages = {167-175},
year = {2006},
issn = {1553-7250},
doi = {https://doi.org/10.1016/S1553-7250(06)32022-3},
url = {https://www.sciencedirect.com/science/article/pii/S1553725006320223},
author = {Kathleen M. Haig and Staci Sutton and John Whittington},
abstract = {Article-at-a-Glance
Background
The importance of sharing a common mental model in communication prompted efforts to spread the use of the SBAR (Situation, Background, Assessment, and Recommendation) tool at OSF St. Joseph Medical Center, Bloomington, Illinois.
Case Study
An elderly patient was on warfarin sodium (Coumadin) 2.5mg daily. The nurse received a call from the lab regarding an elevated international normalized ratio (INR) but did not write down the results (she was providing care to another patient). On the basis of the previous lab cumulative summary, the physician increased the warfarin dose for the patient; a dangerously high INR resulted.
Actions Taken
The medical center initiated a collaborative to implement the use of the SBAR communication tool. Education was incorporated into team resource management training and general orientation. Tools included SBAR pocket cards for clinicians and laminated SBAR “cheat sheets” posted at each phone. SBAR became the communication methodology from leadership to the microsystem in all forms of reporting.
Discussion
Staff adapted quickly to the use of SBAR, although hesitancy was noted in providing the “recommendation” to physicians. Medical staff were encouraged to listen for the SBAR components and encourage staff to share their recommendation if not initially provided.}
}


Donadello, I., Dragoni, M. (2021). SeXAI: A Semantic Explainable Artificial Intelligence Framework. In: Baldoni, M., Bandini, S. (eds) AIxIA 2020 – Advances in Artificial Intelligence. AIxIA 2020. Lecture Notes in Computer Science(), vol 12414. Springer, Cham. https://doi.org/10.1007/978-3-030-77091-4_4

Podder V, Lew V, Ghassemzadeh S. SOAP Notes. [Updated 2023 Aug 28]. In: StatPearls [Internet]. Treasure Island (FL): StatPearls Publishing; 2025 Jan-. Available from: https://www.ncbi.nlm.nih.gov/books/NBK482263/

@article{GOEL2022105587,
title = {The effect of machine learning explanations on user trust for automated diagnosis of COVID-19},
journal = {Computers in Biology and Medicine},
volume = {146},
pages = {105587},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105587},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522003791},
author = {Kanika Goel and Renuka Sindhgatta and Sumit Kalra and Rohan Goel and Preeti Mutreja},
keywords = {Medical diagnosis, Machine learning explanations, Application-oriented evaluation, User trust},
abstract = {Recent years have seen deep neural networks (DNN) gain widespread acceptance for a range of computer vision tasks that include medical imaging. Motivated by their performance, multiple studies have focused on designing deep convolutional neural network architectures tailored to detect COVID-19 cases from chest computerized tomography (CT) images. However, a fundamental challenge of DNN models is their inability to explain the reasoning for a diagnosis. Explainability is essential for medical diagnosis, where understanding the reason for a decision is as important as the decision itself. A variety of algorithms have been proposed that generate explanations and strive to enhance users' trust in DNN models. Yet, the influence of the generated machine learning explanations on clinicians' trust for complex decision tasks in healthcare has not been understood. This study evaluates the quality of explanations generated for a deep learning model that detects COVID-19 based on CT images and examines the influence of the quality of these explanations on clinicians’ trust. First, we collect radiologist-annotated explanations of the CT images for the diagnosis of COVID-19 to create the ground truth. We then compare ground truth explanations with machine learning explanations. Our evaluation shows that the explanations produced. by different algorithms were often correct (high precision) when compared to the radiologist annotated ground truth but a significant number of explanations were missed (significantly lower recall). We further conduct a controlled experiment to study the influence of machine learning explanations on clinicians' trust for the diagnosis of COVID-19. Our findings show that while the clinicians’ trust in automated diagnosis increases with the explanations, their reliance on the diagnosis reduces as clinicians are less likely to rely on algorithms that are not close to human judgement. Clinicians want higher recall of the explanations for a better understanding of an automated diagnosis system.}
}

@article{pourian2025elements,
  title={The Elements of Style for Interruptive Electronic Health Record Alerts},
  author={Pourian, Jessica J and Blebea, Catherine and Subramanian, Charumathi R and Auerbach, Andrew and Khanna, Raman},
  journal={Applied Clinical Informatics},
  volume={16},
  number={02},
  pages={402--408},
  year={2025},
  publisher={Georg Thieme Verlag KG}
}

Derksen, C., Walter, F.M., Akbar, A.B. et al. The implementation challenge of computerised clinical decision support systems for the detection of disease in primary care: systematic review and recommendations. Implementation Sci 20, 33 (2025). https://doi.org/10.1186/s13012-025-01445-4

@misc{peng2025treeofreasoningcomplexmedicaldiagnosis,
      title={Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree}, 
      author={Qi Peng and Jialin Cui and Jiayuan Xie and Yi Cai and Qing Li},
      year={2025},
      eprint={2508.03038},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.03038}, 
}

@misc{ozgun2025trustworthyaipsychotherapymultiagent,
      title={Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis}, 
      author={Mithat Can Ozgun and Jiahuan Pei and Koen Hindriks and Lucia Donatelli and Qingzhi Liu and Junxiao Wang},
      year={2025},
      eprint={2508.11398},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2508.11398}, 
}