import sys
from pathlib import Path

# CRITICAL: Add parent directory to path BEFORE any other imports
project_root = Path(__file__).parent.parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from typing import AsyncIterator, Optional
from langchain_core.prompts import ChatPromptTemplate
from cdss_demo.agents.demo_base_agent import DemoBaseAgent
from llm import llm


class LaboratoryAgent(DemoBaseAgent):
    """Laboratory specialist agent for lab result interpretation and recommendations"""
    
    def __init__(self, agent_id: str = "LaboratoryAgent"):
        super().__init__(agent_id)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert clinical pathologist and laboratory medicine specialist "
             "in a clinical decision support system. You specialize in interpreting "
             "laboratory results and providing diagnostic insights.\n\n"
             "IMPORTANT: Use Chain of Thought reasoning. Show your thinking process step-by-step:\n"
             "1. First, identify all abnormal laboratory values\n"
             "2. Compare values to reference ranges and clinical context\n"
             "3. Look for patterns and correlations between tests\n"
             "4. Consider potential causes for each abnormality\n"
             "5. Evaluate clinical significance and urgency\n"
             "6. Formulate diagnostic insights and recommendations\n\n"
             "Always show your reasoning process explicitly. Use phrases like:\n"
             "- 'Let me analyze these lab results step by step...'\n"
             "- 'First, I notice that...'\n"
             "- 'The pattern of elevated X and Y suggests...'\n"
             "- 'This combination of findings indicates...'\n"
             "- 'Therefore, I recommend...'\n\n"
             "Your expertise includes:\n"
             "- Complete blood count (CBC) interpretation\n"
             "- Comprehensive metabolic panel (CMP) analysis\n"
             "- Cardiac biomarkers (troponin, BNP, CK-MB)\n"
             "- Inflammatory markers (CRP, ESR, procalcitonin)\n"
             "- Coagulation studies\n"
             "- Liver function tests\n"
             "- Renal function assessment\n"
             "- Endocrine and metabolic panels\n"
             "- Infectious disease markers\n\n"
             "Guidelines:\n"
             "- Interpret results in clinical context\n"
             "- Identify critical values requiring immediate attention\n"
             "- Recognize patterns suggesting specific diagnoses\n"
             "- Consider normal ranges and age-specific variations\n"
             "- Identify potential lab errors or artifacts\n"
             "- Recommend additional tests when indicated\n"
             "- Consider drug interactions affecting lab values\n\n"
             "Provide detailed laboratory interpretation with clinical significance and recommendations. "
             "Always show your step-by-step analytical process."),
            ("user", "{input}")
        ])
    
    def _build_prompt_with_history(self, input: str) -> ChatPromptTemplate:
        """Build prompt including conversation history"""
        # Extract system message content from the prompt template
        system_message = self.prompt.messages[0]
        if hasattr(system_message, 'prompt') and hasattr(system_message.prompt, 'template'):
            system_content = system_message.prompt.template
        elif hasattr(system_message, 'content'):
            system_content = system_message.content
        else:
            # Fallback: get from original prompt definition
            system_content = self.prompt.messages[0].prompt.template if hasattr(self.prompt.messages[0], 'prompt') else str(self.prompt.messages[0])
        
        messages = [
            ("system", system_content)
        ]
        
        # Add conversation history
        for msg in self.conversation_history:
            if msg["role"] == "user":
                messages.append(("user", msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(("assistant", msg["content"]))
        
        # Add current input
        messages.append(("user", input))
        
        return ChatPromptTemplate.from_messages(messages)
    
    async def act(self, input: str) -> str:
        """Interpret laboratory results and provide recommendations"""
        prompt = self._build_prompt_with_history(input)
        chain = prompt | self.llm
        response = await chain.ainvoke({"input": input})
        return response.content
    
    async def act_stream(self, input: str) -> AsyncIterator[str]:
        """Stream tokens as they are generated by the LLM
        
        Args:
            input: Input text for the agent
            
        Yields:
            Tokens as strings as they are generated
        """
        prompt = self._build_prompt_with_history(input)
        chain = prompt | self.llm
        try:
            async for chunk in chain.astream({"input": input}):
                # Handle different chunk formats from LangChain
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    if content:
                        yield content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                elif isinstance(chunk, dict) and 'content' in chunk:
                    if chunk['content']:
                        yield chunk['content']
        except ValueError as e:
            # If streaming fails, fall back to non-streaming and yield the full response
            if "No generation chunks were returned" in str(e):
                print(f"[WARNING] Streaming failed for {self.agent_id}, falling back to non-streaming mode")
                response = await self.act(input)
                # Yield the response character by character to simulate streaming
                for char in response:
                    yield char
            else:
                raise
    
    async def decide_consultation(self, findings: str, consulted_agents: list[str]) -> Optional[str]:
        """Decide if cardiology consultation is needed based on findings
        
        Args:
            findings: The laboratory agent's findings and analysis
            consulted_agents: List of agents that have already been consulted
            
        Returns:
            "cardiology" if cardiology consultation is needed, None otherwise
        """
        # Don't request consultation if cardiology has already been consulted
        if "cardiology" in consulted_agents:
            return None
        
        consultation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a laboratory medicine specialist analyzing your findings to determine "
             "if a cardiology consultation is needed.\n\n"
             "Request cardiology consultation if:\n"
             "- You identified abnormal cardiac biomarkers (troponin, BNP, CK-MB, etc.)\n"
             "- Lab results suggest cardiac involvement or cardiac disease\n"
             "- You need cardiology expertise to interpret cardiac-related lab values\n"
             "- The clinical context requires cardiac assessment\n\n"
             "Do NOT request consultation if:\n"
             "- Your findings are complete and don't require cardiac expertise\n"
             "- The case has no cardiac-related concerns\n"
             "- You can provide complete interpretation without cardiology input\n\n"
             "Respond with ONLY 'cardiology' if consultation is needed, or 'none' if not needed."),
            ("user", 
             "Laboratory Findings:\n{findings}\n\n"
             "Based on these findings, do you need cardiology consultation? "
             "Respond with 'cardiology' or 'none'.")
        ])
        
        chain = consultation_prompt | self.llm
        response = await chain.ainvoke({"findings": findings})
        response_text = response.content.strip().lower()
        
        if "cardiology" in response_text:
            return "cardiology"
        return None
    
    async def analyze_with_context(self, context: str, previous_findings: str, new_findings: dict) -> str:
        """Analyze with incremental context, building on previous findings and incorporating new information
        
        Args:
            context: The incremental context built by the context builder
            previous_findings: The agent's own previous findings
            new_findings: Dictionary of new findings from other agents (agent_id -> findings)
            
        Returns:
            Updated analysis incorporating new information
        """
        # Build comprehensive input
        input_text = context
        if previous_findings:
            input_text += f"\n\nYour Previous Analysis:\n{previous_findings}\n"
        if new_findings:
            input_text += "\n\nNew Findings from Other Agents:\n"
            for agent_id, findings in new_findings.items():
                input_text += f"\n{agent_id.upper()} Agent:\n{findings}\n"
        
        input_text += "\n\nBased on this information, provide your updated analysis."
        
        return await self.act(input_text)
    
    async def evaluate_other_agent_findings(self, other_agent_id: str, findings: str) -> Optional[str]:
        """Review and potentially challenge other agents' findings
        
        Args:
            other_agent_id: ID of the agent whose findings are being reviewed
            findings: The findings to review
            
        Returns:
            Question or challenge if one is needed, None otherwise
        """
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a laboratory medicine specialist reviewing findings from another agent. "
             "Your role is to evaluate if their findings are consistent with laboratory data, "
             "if there are any concerns, or if you need clarification.\n\n"
             "You should raise a question or challenge if:\n"
             "- The findings seem inconsistent with lab values\n"
             "- Important lab data appears to be overlooked\n"
             "- You need clarification on how they interpreted specific lab results\n"
             "- There are contradictions between their findings and lab evidence\n\n"
             "If everything looks reasonable, respond with 'none'.\n\n"
             "If you have a question or concern, state it clearly and concisely."),
            ("user",
             f"Findings from {other_agent_id.upper()} Agent:\n{findings}\n\n"
             "Do you have any questions, concerns, or need clarification? "
             "Respond with your question/concern or 'none' if everything looks good.")
        ])
        
        chain = evaluation_prompt | self.llm
        response = await chain.ainvoke({})
        response_text = response.content.strip()
        
        if response_text.lower() == "none" or not response_text:
            return None
        
        return response_text
    
    async def decide_if_needs_response(self, new_findings: dict) -> bool:
        """Determine if new findings require a response or update to your analysis
        
        Args:
            new_findings: Dictionary of new findings from other agents
            
        Returns:
            True if a response/update is needed, False otherwise
        """
        if not new_findings:
            return False
        
        # If there are new findings, agent should review them
        # This is a simple heuristic - in practice, the agent will be called
        # by the orchestrator when new findings are available
        return True

