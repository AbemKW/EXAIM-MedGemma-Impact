import sys
from pathlib import Path

# CRITICAL: Add parent directory to path BEFORE any other imports
project_root = Path(__file__).parent.parent.parent.resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from typing import AsyncIterator, Optional
from langchain_core.prompts import ChatPromptTemplate
from cdss_demo.agents.demo_base_agent import DemoBaseAgent
from llm import llm


class CardiologyAgent(DemoBaseAgent):
    """Cardiology specialist agent for cardiac assessment and recommendations"""
    
    def __init__(self, agent_id: str = "CardiologyAgent"):
        super().__init__(agent_id)
        self.llm = llm
        self.prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are an expert cardiologist in a clinical decision support system. "
             "You specialize in cardiac assessment, interpretation of cardiac tests, "
             "and cardiovascular disease management.\n\n"
             "IMPORTANT: Use Chain of Thought reasoning. Show your thinking process step-by-step:\n"
             "1. First, review the cardiac history and risk factors\n"
             "2. Analyze cardiac symptoms and physical exam findings\n"
             "3. Interpret cardiac biomarkers and diagnostic tests\n"
             "4. Consider differential diagnoses systematically\n"
             "5. Evaluate urgency and severity\n"
             "6. Formulate evidence-based recommendations\n\n"
             "Always show your reasoning process explicitly. Use phrases like:\n"
             "- 'Let me think through this cardiac case step by step...'\n"
             "- 'First, I need to assess the cardiac risk factors...'\n"
             "- 'The elevated troponin suggests...'\n"
             "- 'Given these findings, I conclude...'\n"
             "- 'Therefore, my recommendation is...'\n\n"
             "Your expertise includes:\n"
             "- Cardiac history interpretation and risk stratification\n"
             "- ECG interpretation and cardiac imaging\n"
             "- Cardiac biomarker analysis (troponin, BNP, etc.)\n"
             "- Heart failure, arrhythmias, coronary artery disease\n"
             "- Hypertension and cardiovascular risk factors\n"
             "- Cardiac medication recommendations\n\n"
             "Guidelines:\n"
             "- Follow ACC/AHA clinical practice guidelines\n"
             "- Assess cardiovascular risk factors comprehensively\n"
             "- Consider differential diagnoses systematically\n"
             "- Provide evidence-based recommendations\n"
             "- Identify urgent cardiac conditions requiring immediate attention\n"
             "- Consider patient comorbidities and medications\n\n"
             "Provide detailed cardiac assessment with clear reasoning and recommendations. "
             "Always show your step-by-step thought process."),
            ("user", "{input}")
        ])
    
    def _build_prompt_with_history(self, input: str) -> ChatPromptTemplate:
        """Build prompt including conversation history"""
        # Extract system message content from the prompt template
        system_message = self.prompt.messages[0]
        if hasattr(system_message, 'prompt') and hasattr(system_message.prompt, 'template'):
            system_content = system_message.prompt.template
        elif hasattr(system_message, 'content'):
            system_content = system_message.content
        else:
            # Fallback: get from original prompt definition
            system_content = self.prompt.messages[0].prompt.template if hasattr(self.prompt.messages[0], 'prompt') else str(self.prompt.messages[0])
        
        messages = [
            ("system", system_content)
        ]
        
        # Add conversation history
        for msg in self.conversation_history:
            if msg["role"] == "user":
                messages.append(("user", msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(("assistant", msg["content"]))
        
        # Add current input
        messages.append(("user", input))
        
        return ChatPromptTemplate.from_messages(messages)
    
    async def act(self, input: str) -> str:
        """Perform cardiac assessment and provide recommendations"""
        prompt = self._build_prompt_with_history(input)
        chain = prompt | self.llm
        response = await chain.ainvoke({"input": input})
        return response.content
    
    async def act_stream(self, input: str) -> AsyncIterator[str]:
        """Stream tokens as they are generated by the LLM
        
        Args:
            input: Input text for the agent
            
        Yields:
            Tokens as strings as they are generated
        """
        prompt = self._build_prompt_with_history(input)
        chain = prompt | self.llm
        try:
            async for chunk in chain.astream({"input": input}):
                # Handle different chunk formats from LangChain
                if hasattr(chunk, 'content'):
                    content = chunk.content
                    if content:
                        yield content
                elif isinstance(chunk, str) and chunk:
                    yield chunk
                elif isinstance(chunk, dict) and 'content' in chunk:
                    if chunk['content']:
                        yield chunk['content']
        except ValueError as e:
            # If streaming fails, fall back to non-streaming and yield the full response
            if "No generation chunks were returned" in str(e):
                print(f"[WARNING] Streaming failed for {self.agent_id}, falling back to non-streaming mode")
                response = await self.act(input)
                # Yield the response character by character to simulate streaming
                for char in response:
                    yield char
            else:
                raise
    
    async def decide_consultation(self, findings: str, consulted_agents: list[str]) -> Optional[str]:
        """Decide if laboratory consultation is needed based on findings
        
        Args:
            findings: The cardiology agent's findings and analysis
            consulted_agents: List of agents that have already been consulted
            
        Returns:
            "laboratory" if laboratory consultation is needed, None otherwise
        """
        # Don't request consultation if laboratory has already been consulted
        if "laboratory" in consulted_agents:
            return None
        
        consultation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a cardiologist analyzing your findings to determine "
             "if a laboratory consultation is needed.\n\n"
             "Request laboratory consultation if:\n"
             "- You need additional lab values to complete your cardiac assessment\n"
             "- You need interpretation of specific lab results (cardiac biomarkers, metabolic panels, etc.)\n"
             "- Lab values are mentioned but not fully analyzed in the case\n"
             "- You need laboratory expertise to interpret abnormal values\n"
             "- The clinical context requires detailed lab interpretation\n\n"
             "Do NOT request consultation if:\n"
             "- Your findings are complete and don't require lab interpretation\n"
             "- The case has no laboratory-related concerns\n"
             "- You can provide complete assessment without laboratory input\n\n"
             "Respond with ONLY 'laboratory' if consultation is needed, or 'none' if not needed."),
            ("user", 
             "Cardiology Findings:\n{findings}\n\n"
             "Based on these findings, do you need laboratory consultation? "
             "Respond with 'laboratory' or 'none'.")
        ])
        
        chain = consultation_prompt | self.llm
        response = await chain.ainvoke({"findings": findings})
        response_text = response.content.strip().lower()
        
        if "laboratory" in response_text:
            return "laboratory"
        return None
    
    async def analyze_with_context(self, context: str, previous_findings: str, new_findings: dict) -> str:
        """Analyze with incremental context, building on previous findings and incorporating new information
        
        Args:
            context: The incremental context built by the context builder
            previous_findings: The agent's own previous findings
            new_findings: Dictionary of new findings from other agents (agent_id -> findings)
            
        Returns:
            Updated analysis incorporating new information
        """
        # Build comprehensive input
        input_text = context
        if previous_findings:
            input_text += f"\n\nYour Previous Analysis:\n{previous_findings}\n"
        if new_findings:
            input_text += "\n\nNew Findings from Other Agents:\n"
            for agent_id, findings in new_findings.items():
                input_text += f"\n{agent_id.upper()} Agent:\n{findings}\n"
        
        input_text += "\n\nBased on this information, provide your updated analysis."
        
        return await self.act(input_text)
    
    async def evaluate_other_agent_findings(self, other_agent_id: str, findings: str) -> Optional[str]:
        """Review and potentially challenge other agents' findings
        
        Args:
            other_agent_id: ID of the agent whose findings are being reviewed
            findings: The findings to review
            
        Returns:
            Question or challenge if one is needed, None otherwise
        """
        evaluation_prompt = ChatPromptTemplate.from_messages([
            ("system",
             "You are a cardiologist reviewing findings from another agent. "
             "Your role is to evaluate if their findings are consistent with cardiac data, "
             "if there are any concerns, or if you need clarification.\n\n"
             "You should raise a question or challenge if:\n"
             "- The findings seem inconsistent with cardiac biomarkers or tests\n"
             "- Important cardiac data appears to be overlooked\n"
             "- You need clarification on how they interpreted specific cardiac findings\n"
             "- There are contradictions between their findings and cardiac evidence\n\n"
             "If everything looks reasonable, respond with 'none'.\n\n"
             "If you have a question or concern, state it clearly and concisely."),
            ("user",
             f"Findings from {other_agent_id.upper()} Agent:\n{findings}\n\n"
             "Do you have any questions, concerns, or need clarification? "
             "Respond with your question/concern or 'none' if everything looks good.")
        ])
        
        chain = evaluation_prompt | self.llm
        response = await chain.ainvoke({})
        response_text = response.content.strip()
        
        if response_text.lower() == "none" or not response_text:
            return None
        
        return response_text
    
    async def decide_if_needs_response(self, new_findings: dict) -> bool:
        """Determine if new findings require a response or update to your analysis
        
        Args:
            new_findings: Dictionary of new findings from other agents
            
        Returns:
            True if a response/update is needed, False otherwise
        """
        if not new_findings:
            return False
        
        # If there are new findings, agent should review them
        # This is a simple heuristic - in practice, the agent will be called
        # by the orchestrator when new findings are available
        return True

