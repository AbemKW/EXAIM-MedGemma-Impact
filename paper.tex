\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{evals/data/metrics/figures/final/}}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}

% Churkin Protocol: Color Palette (Harmonious & Minimal)
\definecolor{churkinpurple}{RGB}{94,45,121}    % #5E2D79 (for State)
\definecolor{statusgreen}{RGB}{34,139,34}      % #228B22 (Green for True - Success)
\definecolor{statusred}{RGB}{255,99,71}       % #FF6347 (Light Red/Tomato for False - Stop/Suppressed)

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{EXAIM: Explainable AI Middleware for Real-Time Multi-Agent Clinical Decision Support}

\author{\IEEEauthorblockN{1\textsuperscript{st} Author Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email@example.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Author Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email@example.com}
}

\maketitle

\begin{abstract}
 Clinical decision support systems (CDSS) increasingly leverage multi-agent large language model (LLM) architectures to decompose complex diagnostic reasoning into specialized components. However, these systems generate verbose, interleaved reasoning traces that are difficult for clinicians to interpret in real time, limiting transparency and clinical adoption. We present EXAIM (Explainable AI Middleware), a novel middleware architecture that transforms live multi-agent reasoning traces into structured, concise summaries aligned with clinical documentation schemas. EXAIM employs a three-layer pipeline: (1) TokenGate, a syntax-aware pre-buffer that regulates token flow using configurable word thresholds and boundary detection; (2) BufferAgent, an LLM-powered semantic boundary detector that uses structured output analysis to trigger summarization based on completeness, relevance, novelty, and topic coherence; and (3) SummarizerAgent, a schema-constrained compressor that generates summaries aligned with clinical communication frameworks (SBAR/SOAP) while enforcing evidence-based character limits. Unlike post-hoc explanation methods, EXAIM operates incrementally on live reasoning streams, improving information density in the clinician-facing update stream while preserving diagnostic transparency. Our architecture addresses critical gaps in CDSS explainability by providing process-level narrative transparency rather than feature-importance visualizations, supporting real-time clinical workflows, and maintaining multi-agent attribution. The system's modular design enables integration with existing multi-agent CDSS frameworks while remaining agnostic to specific clinical domains or agent implementations. This work contributes a reproducible, empirically evaluated approach to explainable multi-agent clinical AI, with implications for human-AI collaboration in time-sensitive medical decision-making.
\end{abstract}

\begin{IEEEkeywords}
Clinical decision support systems, explainable artificial intelligence, multi-agent systems, large language models, real-time summarization, middleware architecture, human-AI collaboration
\end{IEEEkeywords}

\section{Introduction}

Clinical decision support systems (CDSS) increasingly employ multi-agent large language model (LLM) architectures to decompose complex diagnostic reasoning into specialized components such as information retrieval, differential diagnosis, uncertainty estimation, and safety review \cite{b1,b2,b3,b4}. While these architectures improve diagnostic performance and robustness \cite{b3,b4}, they introduce a critical usability challenge: multi-agent systems generate long, interleaved reasoning traces that are difficult for clinicians to interpret during time-sensitive decision-making \cite{b5,b16,b17}.



Existing approaches to explainability in clinical AI predominantly rely on post-hoc methods such as feature importance visualizations, attention maps, or static textual explanations generated after inference completion \cite{b6}. Although valuable for auditing model behavior, these methods are poorly aligned with real-time clinical workflows \cite{b7,b12}. They expose internal model mechanics rather than the evolving clinical narrative \cite{b8,b9}. Furthermore, traditional explainability approaches fail to account for distributed reasoning and high-velocity, redundant trace emissions that increase output volume and update frequency in clinician-facing interfaces, often resulting in verbose or redundant information that does not reflect the specialized roles of distinct agents \cite{b10,b11}.



Conversational interfaces for multi-agent systems typically rely on turn-based interaction paradigms, surfacing outputs only at agent turn boundaries. In human-human communication, turn-taking corresponds to semantic completion; however, in multi-agent LLM systems, a single turn may contain multiple topic shifts, partial hypotheses, or exploratory reasoning. Treating turns as semantic units leads to lossy compression, redundancy, and reduced faithfulness when reasoning traces are summarized for clinical consumption. These limitations highlight the need for a real-time, process-level explainability mechanism that operates during reasoning rather than after it. Such a mechanism must regulate information flow, identify semantically meaningful update points, and present structured summaries that align with clinical communication practices while preserving attribution and uncertainty \cite{b13}.

\textbf{Research Questions}

We investigate whether an event-driven middleware can improve the efficiency and faithfulness of real-time multi-agent transparency under a fixed output contract:

\textbf{RQ1:} Can event-driven, schema-constrained summarization increase semantic trace coverage efficiency and reduce redundant updates compared to turn-based and fixed-chunk baselines, without modifying upstream diagnostic agents?

To operationalize RQ1, we study four sub-questions aligned with our evaluation metrics:

\begin{itemize}
\item \textbf{RQ1a (Coverage):} Does EXAIM improve global semantic trace coverage under identical schema and length constraints?
\item \textbf{RQ1b (Redundancy):} Does EXAIM reduce redundant and low-novelty updates while regulating update frequency?
\item \textbf{RQ1c (Faithfulness):} Does EXAIM preserve contract-grounded faithfulness when limited continuity across summaries is permitted?
\item \textbf{RQ1d (Overhead):} What latency and LLM usage overhead does semantic event detection introduce relative to simpler triggers?
\end{itemize}



This study proposes EXAIM (Explainable AI Middleware), a real-time summarization architecture designed to bridge the gap between multi-agent reasoning complexity and clinical information needs. Building on recent advances in incremental summarization \cite{b14,b15}, EXAIM operates as an intermediary layer between upstream diagnostic agents and clinician-facing interfaces, continuously monitoring reasoning streams and generating concise, structured summaries at clinically meaningful moments. Rather than relying on fixed turn boundaries or static thresholds, EXAIM employs semantic event detection to trigger updates only when new, relevant, and non-redundant clinical information emerges.



EXAIM is implemented as a modular, domain-agnostic middleware composed of three components: (i) a syntax-aware stream regulator that segments high-velocity token streams into semantically coherent units, (ii) a semantic buffering agent that detects topic shifts, novelty, and critical events, and (iii) a schema-constrained summarization agent that produces structured updates aligned with established clinical communication frameworks. The architecture supports incremental transparency, regularizes update frequency while suppressing redundant/low-value updates, and preserves multi-agent attribution without requiring modification of upstream diagnostic models. This approach leverages evidence that strict length constraints and structured documentation (e.g., SOAP/SBAR) support predictable, scannable presentation formats in clinical communication \cite{b18,b19,b20}.



We evaluate EXAIM using replayed multi-agent clinical reasoning traces and compare it against turn-based and heuristic baselines through a controlled ablation study. Results demonstrate that semantic buffering significantly improves trace coverage and faithfulness while reducing redundant updates, at the cost of modest computational overhead. These findings suggest that middleware-level intervention is a viable and effective strategy for enabling real-time explainability in multi-agent clinical AI systems.

This paper makes the following contributions:
\begin{itemize}
\item \textbf{Middleware Architecture for Real-Time Explainability:} We propose EXAIM, a modular middleware architecture that transforms streaming multi-agent reasoning traces into structured, clinician-aligned summaries without modifying upstream diagnostic agents.
\item \textbf{Semantic Event–Driven Summarization:} We introduce a semantic buffering mechanism that detects topic shifts, novelty, and critical clinical events to trigger summaries based on meaning rather than agent turn boundaries.
\item \textbf{Schema-Constrained Clinical Summarization:} We operationalize process-level transparency through structured summaries aligned with SBAR/SOAP communication patterns, preserving agent attribution and uncertainty while enforcing strict brevity constraints.
\item \textbf{Empirical Evaluation of Streaming Explainability Trade-offs:} Through a controlled ablation study, we quantify the impact of semantic buffering on coverage, redundancy, faithfulness, and computational cost in multi-agent clinical reasoning streams.
\end{itemize}

\section{Literature Review}
\subsection{Clinical Decision Support Systems and Explainability}
Clinical decision support systems (CDSS) have demonstrated potential to improve diagnostic accuracy, guideline adherence, and patient safety \cite{b1,b2}, yet their real-world adoption remains constrained by usability and workflow integration challenges \cite{b12}. Prior studies emphasize that excessive alerting, poor timing, and opaque system behavior contribute to clinician frustration and alert fatigue, limiting trust and sustained use \cite{b13}.



As CDSS increasingly incorporate machine learning and large language models (LLMs), explainability has emerged as a critical requirement for safe deployment in clinical environments. Explainable AI (XAI) approaches in CDSS have largely focused on post-hoc interpretation methods, including feature importance rankings, attention visualization, and surrogate model explanations \cite{b6}. While these techniques provide insight into model behavior, they primarily expose internal model mechanics rather than supporting clinical sensemaking \cite{b8}. Empirical studies show that clinicians often prefer concise, narrative explanations grounded in clinical reasoning over probabilistic or feature-based explanations, particularly in time-sensitive contexts \cite{b9}. This misalignment between existing XAI techniques and clinical information needs motivates alternative approaches to explainability that prioritize process transparency and communication fidelity.

\subsection{Multi-Agent Systems for Clinical Reasoning}
Multi-agent LLM architectures have gained traction as a means of decomposing complex clinical reasoning tasks across specialized agents responsible for information retrieval, hypothesis generation, verification, and safety assessment \cite{b3,b4}. Prior work demonstrates that distributing reasoning across agents can improve diagnostic performance, robustness, and oversight compared to monolithic models \cite{b3}. These systems also offer theoretical advantages for transparency, as individual agents can be assigned interpretable roles within the diagnostic process \cite{b10,b11}.



However, multi-agent systems introduce new explainability challenges. Agent interactions generate long, interleaved reasoning traces that include exploratory hypotheses, internal deliberation, and repetitive grounding statements \cite{b5,b16}. While some systems explicitly expose agent reasoning to users, these traces are typically verbose and require post-hoc review, limiting their practical utility during live clinical decision-making \cite{b17}. Existing multi-agent frameworks largely assume that transparency is achieved by exposing reasoning in full, rather than by managing how and when information is presented to clinicians.

\subsection{Turn-Based Interfaces and Streaming Reasoning}
Most conversational and multi-agent AI systems adopt turn-based interaction paradigms, surfacing outputs only when an agent completes a turn. This abstraction is inherited from human dialogue systems, where turn-taking often corresponds to semantic completion. In multi-agent LLM systems, however, a single turn may contain multiple topic shifts, partial inferences, or exploratory reasoning branches. Treating turns as semantic units can therefore result in lossy compression, redundancy, and reduced faithfulness when summarization is applied.

Recent work on streaming and incremental reasoning highlights the limitations of turn-based interfaces in high-velocity information environments \cite{b14,b15}. Studies in meeting summarization, customer support, and medical dialogue demonstrate that incremental updates triggered by semantic boundaries rather than fixed intervals improve information retention and regularize update frequency \cite{b23}. These findings suggest that real-time systems require adaptive mechanisms to regulate information flow based on content, not generation structure.



\subsection{Clinical Summarization and Structured Communication}
Clinical summarization research provides important foundations for managing information overload in healthcare settings. Prior work demonstrates that structured summaries aligned with clinical documentation standards, such as SBAR and SOAP, support predictable, scannable presentation formats, reduce errors, and support handoff communication \cite{b19,b20}. Large language models have shown strong performance in generating clinical summaries when explicit length and structure constraints are enforced \cite{b18}.


Recent studies further indicate that incremental or streaming summarization can outperform post-hoc summarization by preserving temporal context and reducing the need for retrospective compression \cite{b21}. Additionally, token filtering strategies have been proposed to reduce redundancy in clinical texts \cite{b22}. However, most clinical summarization systems operate on completed conversations or documents and do not address the challenges posed by multi-agent reasoning streams. Specifically, they assume a single narrative source rather than multiple interacting agents producing overlapping and partially redundant content.

\subsection{Middleware Architectures for Clinical AI}
Middleware approaches have been proposed to integrate AI reasoning with electronic health records and clinical workflows, enabling data normalization, orchestration, and post-processing \cite{b24}. These architectures demonstrate the feasibility of inserting intermediate layers between AI models and user interfaces to manage complexity and ensure compliance. However, existing middleware solutions primarily focus on data access, interoperability, or static summarization and do not address real-time explainability for streaming multi-agent reasoning.

To our knowledge, prior systems do not jointly (i) perform semantic event triggering over interleaved multi-agent traces and (ii) emit clinician-aligned schema summaries as a decoupled middleware layer. EXAIM builds insights from XAI, clinical summarization, and streaming systems by introducing a dedicated middleware layer that regulates information flow, detects meaningful reasoning events, and generates clinician-aligned summaries in real time.

\section{Methodology}

This study follows the Design Science Research Methodology (DSRM) \cite{b25}. We mapped the six DSRM steps to our research lifecycle as follows: (1) Problem Identification: We identified the lack of real-time transparency in multi-agent CDSS (Section I). (2) Objectives: We derived design goals for a middleware-based solution (Section IV-A). (3) Design \& Development: We iteratively constructed the EXAIM architecture (Section IV-B). (4) Demonstration: We validated utility via a clinical case walkthrough (Section IV-C). (5) Evaluation: We assessed the artifact using a controlled ablation study (Section V). (6) Communication: This paper serves as the dissemination artifact for the scientific community.

\section{Artifact Design}

\subsection{Design Objectives \& Rationale}

EXAIM is motivated by the observation that existing explainability approaches for clinical AI systems inadequately address the dynamics of real-time, multi-agent reasoning. Rather than optimizing for post-hoc transparency or exhaustive trace exposure, the design of EXAIM prioritizes timely, clinically meaningful communication under strict brevity and update-frequency constraints. This subsection articulates the key design decisions underlying the architecture and their theoretical and practical justification.

\subsection{Middleware-Level Explainability}

A foundational design decision in EXAIM is to implement explainability at the middleware level rather than within individual diagnostic agents. Embedding explainability mechanisms directly into agent prompts or reasoning strategies risks entangling transparency with diagnostic performance, complicating validation and limiting portability across systems. By positioning EXAIM as an intermediary layer, explainability is decoupled from reasoning generation. This separation of concerns enables independent evolution of diagnostic agents and explanation mechanisms, supports integration with heterogeneous multi-agent frameworks, and preserves upstream system behavior. Middleware-level intervention also allows EXAIM to observe the full interaction among agents, rather than isolated reasoning fragments, which is essential for process-level transparency.

\subsection{Semantic-Driven Update Strategy}

EXAIM's update mechanism operates on semantic content rather than structural or mechanical properties, addressing both when to trigger updates and how to evaluate content relevance. Traditional approaches rely on either turn boundaries or fixed thresholds—both of which are poor proxies for semantic progression in multi-agent LLM systems. Turn-based summarization assumes that agent turn boundaries correspond to meaningful semantic transitions, but a single agent turn may contain exploratory reasoning, multiple topic transitions, or internal debate, whereas clinically meaningful changes may occur mid-turn. Similarly, fixed thresholds—such as token counts or time intervals—are computationally simple but insensitive to content, often producing fragmented or repetitive updates.

EXAIM therefore adopts a unified semantic-driven strategy that addresses both timing and content evaluation. For update timing, EXAIM uses an event-driven approach in which summaries are triggered by semantic change rather than generation mechanics. This design aligns updates with clinically salient moments, such as the emergence of new evidence, shifts in diagnostic focus, or safety-critical findings. For content evaluation, EXAIM assesses semantic properties of the reasoning stream—including completeness, relevance, and novelty—rather than relying on heuristic thresholds. This filtering strategy allows the middleware to distinguish between verbosity and progression, suppressing repetitive grounding statements while surfacing genuinely new diagnostic information. By decoupling update timing from turn structure and content evaluation from mechanical thresholds, EXAIM avoids lossy compression and reduces redundant updates, while also enabling adaptation to variability in agent behavior, case complexity, and reasoning style without requiring manual retuning of thresholds for each deployment. Together, these design decisions position semantic content as the primary signal for update decisions, ensuring that transparency mechanisms respond to meaningful clinical progression rather than structural artifacts of the reasoning process.

\subsection{Novelty Gating for Update Stream Control}

A key design insight in EXAIM is that redundancy reduction alone is insufficient for managing update frequency and marginal-information emissions. Multi-agent systems may produce a sequence of distinct yet marginally informative updates that are technically non-redundant but represent low marginal-information emissions. To address this, EXAIM explicitly incorporates novelty as a gating criterion. Novelty is evaluated relative to previously surfaced summaries rather than raw agent output, allowing the system to suppress low-impact incremental updates until sufficient informational value has accumulated. This design trades marginal gains in information coverage for substantial reductions in interruption frequency, aligning with evidence-based principles for alert fatigue mitigation in clinical systems.

\subsection{Evidence-Informed Output Budgeting}

A critical design decision in EXAIM is the enforcement of strict character limits for each field of the summary schema (e.g., 150 characters for Status, 180 for Findings). These constraints are not arbitrary; they represent evidence-informed design heuristics motivated by prior work on concise clinical communication and explanation length, establishing bounded output budgets for schema-constrained streaming summaries.

\textbf{The ``Elements of Style'' for Interruptive Alerts:} Research on EHR usability emphasizes that alerts must be concise and consistently structured to reduce error rates \cite{b13}. Usability guidelines specifically identify ``title brevity'' and ``minimal introductory text'' as key principles for managing update frequency and marginal-information emissions. EXAIM operationalizes this by enforcing hard character caps, establishing a bounded output contract that prevents uncontrolled interface growth and supports predictable length for high-frequency updates.

\textbf{Bounded Output Constraints:} In the domain of Explainable AI (XAI), prior research has examined trade-offs between explanation length and complexity \cite{b6,b8,b9}. Prior XAI research reports preferences for concise explanations focusing on 1–2 key causes over exhaustive causal chains. By capping the Differential/Rationale field at 210 characters, EXAIM enforces bounded output budget constraints, ensuring explanations remain within predictable length bounds suitable for streaming interfaces.

\textbf{Snippet-Based Summarization:} While some clinical benchmarks (e.g., ACI-Bench) focus on full-note generation, EXAIM targets the ``live snippet'' paradigm. Van Veen et al. \cite{b18} demonstrated that for specific clinical queries, explicit length constraints (e.g., ``15 words or less'') serve as a design pattern for controlled output volume. Our character limits effectively enforce this ``snippet'' modality, ensuring the middleware functions as a real-time monitor with stream-safe output budgeting rather than a retrospective documenter.

\subsection{Structured Summarization Aligned with Clinical Communication}

EXAIM enforces a fixed schema inspired by established clinical frameworks (SBAR/SOAP). This structural rigidity is a functional requirement for safety, not just a formatting choice.

\textbf{Consistency for Predictable Output:} Interfaces with clear information organization establish predictable output contracts as a system property. By mapping agent outputs to a consistent, predictable schema (Situation, Background, Assessment), EXAIM enables pattern-based localization through temporal alignment with semantic boundaries, ensuring schema-level consistency and comparability across updates.

\textbf{Simplification for Information Density:} Clinical note simplification research explicitly aims to shorten text to improve information density. EXAIM's Key Findings and Recommendation fields serve this simplification role, acting as a real-time filter that strips away the complex, multi-turn ``reasoning scaffolding'' of the agent swarm to present only the actionable clinical signal.

\subsection{EXAIM Architecture}

EXAIM follows an event-driven architecture composed of three sequential components: (1) a TokenGate for syntax-aware stream regulation, (2) a BufferAgent for semantic event detection, and (3) a SummarizerAgent for schema-constrained synthesis. Each component progressively increases the semantic abstraction of the incoming data while enforcing strictly defined flow-control policies.

\subsubsection{Middleware Placement and Data Flow}

EXAIM is positioned downstream of all diagnostic agents and upstream of the clinician-facing interface. The system operates on a ``push'' model where agents emit tokens asynchronously to the middleware. Unlike turn-based systems that rely on orchestration artifacts (e.g., turn\_end events), EXAIM intercepts the raw token stream, allowing it to operate at a finer granularity than conventional conversational interfaces. The middleware maintains no global diagnostic authority; instead, it functions as an interpretive layer that monitors reasoning progression and emits summaries only when specific semantic thresholds are crossed.

\subsubsection{Syntax-Aware Stream Regulation (TokenGate)}

The first stage of the EXAIM pipeline regulates the high-velocity output of large language models into semantically coherent segments suitable for downstream analysis. Raw token streams produced by LLMs are often fragmented, bursty, and misaligned with linguistic boundaries. Processing such streams directly leads to excessive summarization triggers or incomplete semantic units.

To maintain low latency and avoid dependencies on model-specific tokenizers (e.g., BPE, TikToken), TokenGate operates on whitespace-delimited word counts. The TokenGate accumulates tokens into a per-agent buffer and flushes only when specific structural or temporal conditions are met. These flush policies are prioritized to balance latency with linguistic integrity:

\begin{itemize}
\item \textbf{Boundary-Aware Accumulation:} To prevent severing sentences mid-thought, the gate accumulates tokens until a minimum word threshold (min\_words=60) is met and a boundary cue is detected. We employ a regex pattern that targets sentence-terminating punctuation followed by optional closures: \verb|r"[.?!][)\]\}\'\"]*$"|.
\item \textbf{Silence Detection:} To handle bursty agent behavior, a silence\_timer (1.0s) forces a flush if no new tokens are received, preventing data from becoming ``stuck'' during agent retrieval pauses.
\item \textbf{Safety Valves:} To guarantee upper-bound latency during verbose generation, a hard flush is forced if the buffer exceeds max\_words=100 or a max\_wait\_timeout of 4.0s, regardless of boundary cues.
\end{itemize}

These parameter values were selected through systematic calibration across 625 policy combinations using multi-objective optimization (see Section V.A.1).

\subsubsection{Semantic Buffering and Event Detection (BufferAgent)}

The core innovation of EXAIM lies in its semantic buffering layer, which determines when a clinician-facing update should be generated. Rather than triggering summaries at fixed intervals or agent turn boundaries, EXAIM evaluates the semantic content of buffered segments to detect meaningful reasoning events.

This component analyzes incoming segments using a structured BufferAnalysis state machine, classifying the stream into one of three states:
\begin{itemize}
\item \textbf{SAME\_TOPIC\_CONTINUING:} Elaboration on a previous concept
\item \textbf{TOPIC\_SHIFT:} Movement to a new clinical domain or reasoning phase
\item \textbf{CRITICAL\_ALERT:} High-priority clinical event requiring immediate attention
\end{itemize}

Beyond state classification, the BufferAgent assesses incoming segments along three dimensions:
\begin{itemize}
\item \textbf{Completeness:} whether the segment represents a finished clinical thought rather than an incomplete clause or speculative fragment.
\item \textbf{Relevance:} whether the content contributes to clinically meaningful information, as opposed to internal agent coordination or procedural reasoning.
\item \textbf{Novelty:} whether the information introduces new diagnostic insight relative to previously surfaced summaries.
\end{itemize}

Based on this semantic assessment, the middleware decides whether to trigger summarization or continue accumulating context. Crucially, EXAIM distinguishes verbosity from progression. Multi-agent systems often restate known facts to ground internal reasoning; such repetition is filtered to avoid redundant clinician notifications. At the same time, EXAIM ensures that clinically significant transitions—such as changes in diagnostic focus or emergent safety concerns—are surfaced promptly.

\subsubsection{Schema-Constrained Clinical Summarization (SummarizerAgent)}

When a semantic event is detected, the accumulated context is passed to the summarization layer. This component generates a concise, structured update that reflects the current state of multi-agent reasoning rather than a retrospective summary of the entire case. Summaries are constrained to a fixed schema aligned with established clinical communication frameworks (e.g., SBAR/SOAP). Each summary captures: (i) the current diagnostic status or action, (ii) key clinical findings, (iii) rationale for leading hypotheses, (iv) uncertainty or confidence considerations, (v) recommended next steps, and (vi) attribution of contributing agents.

Strict structural and length constraints are enforced to ensure consistency, readability, and predictable output contracts. The summarization layer is explicitly prohibited from inventing or extrapolating information beyond the provided context. If no new information is available for a given field, the summary reflects this absence rather than repeating prior content. By enforcing schema-level constraints, EXAIM produces updates that are predictable in form, comparable over time, and suitable for integration into clinician-facing dashboards or electronic health record workflows.

\textbf{System Diagram:} [PLACEHOLDER: System architecture diagram showing TokenGate, BufferAgent, and SummarizerAgent components with data flow arrows]

\subsubsection{Case Walkthrough}

To demonstrate EXAIM's semantic filtering and redundancy detection mechanisms, we analyze a specific interaction sequence from a multi-agent reasoning trace (Case ID: 3949). This trace was generated using the MAC Framework (described in Section V-A), which simulates a team of diagnostic agents collaborating on a complex medical case.

\textbf{1. Context State (Pre-Condition)} Prior to this segment, doctor0 and doctor1 had already established Spinocerebellar Ataxia (SCA) as the leading diagnosis. The middleware had generated summaries capturing this consensus and the recommended genetic tests (Log ID: summary\_3). The active buffer was empty.

\textbf{2. The Raw Input Stream} doctor2 then enters the discussion. The agent generates a verbose response concurring with the previous agents and re-stating the established diagnosis.

\textit{Raw Trace (doctor2):} ``\textit{\#\#\# Most Likely Diagnosis: Spinocerebellar Ataxia (SCA): I concur with both Doctor0 and Doctor1 that the most likely diagnosis is spinocerebellar ataxia... The gradual onset at age 30 years also aligns with hereditary SCAs... \#\#\# Differential Diagnoses: 1. Multiple Systems Atrophy (MSA): While I agree that MSA typically presents with...}''

\textbf{3. BufferAgent Logic (Internal Rationale)} While the dashboard remained visually static (see Fig.~\ref{fig:suppression-demo}), the middleware was actively analyzing the stream. As visualized in Fig.~\ref{fig:logic-card}, the system detected semantic redundancy through its multi-stage evaluation process. Despite the input's clinical relevance, the novelty check failed, causing the middleware to suppress the update and prevent redundancy.

\begin{figure}[h]
    \centering
    \begin{tcolorbox}[
        enhanced,
        colback=white,
        colframe=black,
        boxrule=0.5mm,
        arc=2mm,
        drop shadow,
        left=4mm,
        right=4mm,
        top=2mm,
        bottom=2mm,
        title=\textbf{BufferAgent Status [doctor2]},
        fonttitle=\bfseries\normalsize,
        coltitle=black,
        colbacktitle=white,
        halign title=center,
        fontupper=\normalsize
    ]
        \normalsize
        
        \textbf{Input Stream:}
        \vspace{-1mm}
        \begin{quote}
            \itshape "...I concur with both Doctor0 and Doctor1... adding no new information."
        \end{quote}
        \vspace{1.5mm}
        
        \textbf{Decision Flags:}
        \vspace{0.5mm}
        
        \begin{center}
        \begin{tabular}{@{}c@{}}
            \raisebox{5pt}{State} \\
            \tcbox[colback=churkinpurple!15, colframe=churkinpurple, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.5mm, bottom=0.5mm, fontupper=\normalsize]{\textcolor{churkinpurple}{\texttt{SAME\_TOPIC\_CONTINUING}}}
        \end{tabular}
        \end{center}
        
        \vspace{0.1mm}
        \begin{center}
        \begin{tabular}{@{}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{}}
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Complete} \\[-3pt]
                \tcbox[colback=statusgreen!15, colframe=statusgreen, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusgreen}{\textbf{True}}}
            \end{tabular} &
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Relevant} \\[-3pt]
                \tcbox[colback=statusgreen!15, colframe=statusgreen, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusgreen}{\textbf{True}}}
            \end{tabular} &
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Novel} \\[-3pt]
                \tcbox[colback=statusred!15, colframe=statusred, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusred}{\textbf{False}}}
            \end{tabular} \\
        \end{tabular}
        \end{center}
        \vspace{-2mm}
        \begin{center}
            $\downarrow$
        \end{center}
        \vspace{-2mm}
        \begin{center}
            \tcbox[
                enhanced,
                colback=statusred!20,
                colframe=statusred,
                boxrule=0.5mm,
                size=normal,
                fontupper=\bfseries\normalsize,
                left=3mm,
                right=3mm,
                top=0.5mm,
                bottom=0.5mm
            ]{\textcolor{statusred}{Trigger = False (Suppressed)}}
        \end{center}
        
        \vspace{0.1mm}
        \noindent\rule{\linewidth}{0.5mm}
        \vspace{0.5mm}
        
        \textbf{Rationale:} Doctor2 concurs with SCA based on ataxia/dysarthria/MRI... adding no new information. The content is \textcolor{statusgreen}{relevant} as it details planned actions, but it is \textcolor{statusred}{not novel} as these details were already broadly mentioned in previous summaries.
    \end{tcolorbox}
    \caption{BufferAgent System Status Card showing the hierarchical decision logic. Three evaluation conditions (\textcolor{statusgreen}{Complete=True}, \textcolor{statusgreen}{Relevant=True}, \textcolor{statusred}{Novel=False}) combine to produce the final result: \textcolor{statusred}{Trigger=False (Suppressed)}, preventing redundant update generation.}
    \label{fig:logic-card}
\end{figure}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{case_walkthrough/CaseWalkthroughImage.png}
\caption{The EXAIM dashboard during redundancy suppression. The left panel shows the raw, verbose stream from doctor2 concurring with the diagnosis. The right panel remains static (displaying the previous doctor1 update), visually confirming that the middleware successfully filtered the non-novel content.}
\label{fig:suppression-demo}
\end{figure*}

\textbf{4. Outcome} Consequently, EXAIM filtered this update, preventing the generation of a low-value summary. The middleware continued to buffer doctor2's subsequent tokens silently until the agent introduced specific new details regarding ``High-Resolution MRI techniques'' and ``Oligoclonal bands,'' which finally triggered a consolidated, high-density update.

\section{Evaluation}

To isolate the architectural contribution of EXAIM, we employ a ``glass-box'' evaluation methodology using deterministic replays of multi-agent reasoning traces. This approach allows us to stress-test the middleware's flow-control capabilities independent of variations in upstream agent behavior.

\subsection{Experimental Setting}

To evaluate the EXAIM middleware, we utilized the Multi-Agent Conversation (MAC) framework (Chen et al., 2025) as the upstream trace generator. MAC is a multi-agent diagnostic system where diverse doctor agents and a supervisor collaborate to solve medical cases.

For this study, we instrumented the MAC framework to capture the real-time dynamics of these multi-agent interactions. Specifically, we modified the upstream codebase to log granular stream deltas (chunk-level text outputs) and precise generation timestamps (both absolute and relative to the turn start) during the agent reasoning process. This instrumentation enables us to capture the exact cadence of token generation without altering the agents' underlying prompts, roles, or decision-making logic.

The resulting outputs are stored as frozen replay traces. These traces allow us to simulate a live stream deterministically, feeding identical token sequences and timing delays into the EXAIM middleware across all experimental runs. This decoupling ensures that any measured variance in latency or summarization quality is attributable solely to the EXAIM configuration, rather than stochastic variations in the upstream diagnostic agents.

\subsubsection{TokenGate Parameter Calibration}

The TokenGate component requires four configurable parameters: minimum word threshold (min\_words), maximum word threshold (max\_words), silence detection timer (silence\_timer), and maximum wait timeout (max\_wait\_timeout). To ensure reproducible parameter selection, we conducted systematic calibration across a grid of 625 policy combinations evaluated on frozen replay traces. Table~\ref{tab:tokengate-grid} shows the parameter ranges explored in the calibration grid, with validity constraints ensuring min\_words $<$ max\_words and max\_wait\_timeout $\geq$ silence\_timer.

\begin{table}[htbp]
\caption{TokenGate Parameter Calibration Grid}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Values Tested} \\
\midrule
min\_words & 30, 40, 50, 60, 70 \\
max\_words & 80, 100, 120, 140, 160 \\
silence\_timer (seconds) & 1.0, 1.5, 2.0, 2.5, 3.0 \\
max\_wait\_timeout (seconds) & 4.0, 5.0, 6.0, 7.0, 8.0 \\
\bottomrule
\end{tabular}
\label{tab:tokengate-grid}
\end{center}
\end{table}

Each policy was evaluated using metrics that balance latency, computational cost, and semantic coherence: time-to-first-flush (TTFF), average flush frequency per case, median chunk size, spam percentage (flushes below 70\% of min\_words threshold), and worst-case wait times. Policies were filtered through hard constraints: spam percentage $\leq$ 10\%, timer-triggered flushes below min\_words $\leq$ 20\%, median chunk size $\geq$ 50 words, flush count $\leq$ 100 per case, and maximum chunk size $\leq$ 180 words. Derived constraints from trace timing distributions were also applied to ensure policies respect realistic token generation cadences.

From the surviving policies that passed all constraints, selection employed a three-objective Pareto frontier analysis optimizing simultaneously for low TTFF, low flush frequency, and high median chunk size. Objectives were normalized to a [0,1] goodness space using percentile-based bounds computed from survivor policies. The policy with minimum dimension-normalized Euclidean distance to the utopia point (1, 1, 1) was selected, ensuring balanced optimization across all three objectives. This process selected min\_words=60, max\_words=100, silence\_timer=1.0s, and max\_wait\_timeout=4.0s, which balance low latency (rapid first flush), moderate flush frequency (reducing BufferAgent computational overhead), and semantic coherence (sufficient chunk size for meaningful boundary detection).

\subsubsection{EXAIM Implementation Details}

To ensure reproducibility and isolate the middleware's contribution from stochastic model variance, we enforce strict configuration constraints:

\begin{itemize}
\item \textbf{Middleware Model and Deterministic Sampling:} Both the BufferAgent (trigger logic) and SummarizerAgent (synthesis) are powered by Gemini 2.5 Flash Lite (Google), selected for its high throughput and native support for large context windows. All middleware components operate at temperature=0.0 to minimize generation variance and ensure deterministic, reproducible outputs.
\item \textbf{Constraint Enforcement:} Schema validation and character-limit truncation are enforced using Pydantic v2 models with LangChain's structured output (strict=True). The SummarizerAgent employs a three-attempt strategy: initial structured output, retry with rewrite prompt for length violations, and fallback truncation. If validation fails after all attempts, an empty summary is generated and marked as failed (schema\_ok=false) for metrics exclusion, ensuring the system continues operation while tracking failure rates.
\item \textbf{Metric Extraction:} For semantic evaluation, we utilize scispaCy (en\_core\_sci\_sm) with the UMLS Entity Linker to extract canonical medical concepts (CUIs) from both the raw traces and generated summaries.
\end{itemize}

\subsubsection{Ablation Variants}

To isolate the contribution of individual architectural components, we conduct a controlled ablation study comparing the full EXAIM system (V0) against four structural variants: (1) V1 (Turn-End Baseline): Summarization triggered only at agent turn boundaries, without TokenGate or BufferAgent; (2) V2 (No BufferAgent): TokenGate segmentation without semantic buffering, resulting in frequent updates; (3) V3 (Fixed-Chunk): BufferAgent with novelty filtering but fixed-size chunking instead of syntax-aware segmentation; and (4) V4 (No Novelty): Full EXAIM architecture without novelty gating. All variants use the same summarization schema to ensure comparability.

\subsubsection{Evaluation Metrics}

We assessed EXAIM using a hierarchy of ten metrics (M1--M10), distinguishing between primary performance indicators and supplementary contextual measures.

\paragraph{Primary Metrics}

We prioritized five metrics that directly quantify clinical utility, safety, and system efficiency:

\begin{itemize}
    \item \textbf{Strict Faithfulness (M6b):} The semantic alignment between the summary and the ground-truth reasoning trace. Higher scores indicate stronger contract-grounded alignment under a strict concept-realization proxy and are intended to penalize unsupported insertions.
    \item \textbf{Redundancy Reduction (M3):} Measured via Jaccard similarity between consecutive summary updates:
    \begin{equation}
    J(S_i, S_{i+1}) = \frac{|C_i \cap C_{i+1}|}{|C_i \cup C_{i+1}|}
    \end{equation}
    where $C_i$ and $C_{i+1}$ are the sets of unique UMLS concepts (CUIs) extracted from summaries $S_i$ and $S_{i+1}$. Lower scores indicate the successful suppression of repetitive content (solving the verbosity problem).
    \item \textbf{Trace Coverage (M4):} The percentage of key clinical concepts (CUIs) from the raw stream preserved in the summary, ensuring that conciseness does not result in critical information loss.
    \item \textbf{System Latency (M8):} The end-to-end processing time (buffer analysis + summarization generation) to validate the ``real-time'' architectural claim. We report mean, p50 (median), and p95 percentiles across all summary events, as tail latencies are critical for streaming system performance.
    \item \textbf{Schema Compliance (M10):} The rate of successful adherence to the JSON clinical output schema, verifying the system's structural reliability.
\end{itemize}

\paragraph{Supplementary Metrics}

To provide a holistic view of system behavior, we report five additional metrics in Table~\ref{tab:supplementary-metrics}. These include \textbf{Update Frequency (M1)} and \textbf{Token Volume (M2)}, which provide context for the coverage scores; \textbf{Unsupported Content Rate (M5)}, which serves as an inverse proxy for faithfulness; and \textbf{Contextual Faithfulness (M6a)} and \textbf{Budget Efficiency (M7)}, which offer granular insights into token economics.

\subsubsection{Limitations of Concept-Level Faithfulness Metrics in Compressed Clinical Summaries}

Our faithfulness metrics (M6a, M6b) operate at the level of explicit concept realization rather than semantic equivalence or entailment. This design choice introduces a fundamental structural mismatch between what summarization accomplishes and what strict concept-overlap metrics measure.

Schema-constrained summarization necessarily introduces several transformations that systematically reduce recall under strict NER-based concept matching: (1) \textbf{Abbreviation}, where full clinical phrases (e.g., ``acute renal failure'') are compressed to canonical forms (e.g., ``AKI''); (2) \textbf{Normalization}, where hedged or qualified statements (e.g., ``no evidence of infection'') are restated as direct assertions (e.g., ``infection unlikely''); (3) \textbf{Compression}, where modifiers, qualifiers, and redundant descriptors are omitted to meet character limits; and (4) \textbf{Schema remapping}, where clinical facts are reorganized into SBAR/SOAP slots, potentially altering surface forms.

In contrast, UMLS-based NER and concept linking typically assume explicit lexical realization, canonical surface forms, and near-literal mention of clinical entities. These assumptions align poorly with compressed summaries, where semantic content is preserved through paraphrase and structural reorganization rather than verbatim concept repetition. Consequently, a schema-constrained summarizer is guaranteed to under-score on strict concept-overlap metrics, even when the summary accurately represents the clinical content of the source trace.

Despite these limitations, concept-level faithfulness metrics remain informative for our evaluation. While absolute faithfulness scores are depressed by summarization effects, relative differences across ablation variants remain meaningful, as all systems are evaluated under identical extraction and linking assumptions. The metrics effectively capture hallucinated concepts not present in the trace, unsafe insertions (rather than omissions), and systematic differences in grounding quality across architectural variants. However, we do not claim that these metrics constitute a clinical safety guarantee or a substitute for human validation. Future work should incorporate clinician-judged factual consistency, sentence-level entailment verification, semantic similarity models, and task-based evaluation (e.g., decision agreement) to complement automated concept-level assessment.

\subsection{Results}

Table~\ref{tab:primary-metrics} presents the primary performance metrics comparing V0 (Full EXAIM) against all baseline variants. Table~\ref{tab:supplementary-metrics} provides supplementary contextual metrics for comprehensive analysis. Figure~\ref{fig:efficiency-frontier} visualizes the trade-off between interruption frequency and faithfulness, showing V0's position as a favorable operating point in the trade-off space.

\begin{table}[htbp]
\caption{Primary Performance Metrics: V0 vs. Baselines}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{V0} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{V4} \\
\midrule
Faithfulness (M6b) & 0.421 & 0.333 & 0.409 & 0.382 & 0.424 \\
Redundancy (M3) & 0.366 & 0.456 & 0.348 & 0.413 & 0.346 \\
Trace Coverage (M4) & 0.162 & 0.144 & 0.312 & 0.134 & 0.175 \\
Latency (M8, s) & & & & & \\
\quad Mean & 1.28 & 1.03 & 1.14 & 1.07 & 1.37 \\
\quad p50 & 1.22 & 0.96 & 1.06 & 1.04 & 1.33 \\
\quad p95 & 1.84 & 1.59 & 1.91 & 1.52 & 2.12 \\
Schema Compliance (M10) & 0.968 & 0.968 & 0.975 & 0.958 & 0.950 \\
\bottomrule
\end{tabular}
\label{tab:primary-metrics}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Supplementary Contextual Metrics}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{V0} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{V4} \\
\midrule
Update Count (M1) & 11.7 & 8.5 & 46.4 & 9.5 & 16.5 \\
Output Volume (M2, CTU) & 1391 & 1148 & 4313 & 1156 & 1763 \\
Unsupported Rate (M5) & 0.619 & 0.604 & 0.615 & 0.617 & 0.623 \\
Window-Groundedness (M6a) & 0.825 & 0.787 & 0.879 & 0.823 & 0.845 \\
Coverage @ 2000 CTU (M7) & 0.160 & 0.144 & 0.207 & 0.132 & 0.166 \\
\bottomrule
\end{tabular}
\label{tab:supplementary-metrics}
\end{center}
\end{table}

\subsubsection{Full EXAIM vs. Turn-Based Baseline}

Full EXAIM (V0) consistently outperforms the turn-based baseline (V1) across core explainability metrics. As discussed in Section V-A.5, absolute faithfulness scores are systematically depressed by summarization effects (abbreviation, normalization, compression, schema remapping); however, relative comparisons across variants remain informative under identical extraction assumptions.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{efficiency_frontier/efficiency_frontier.pdf}
\caption{Trade-off between interruption frequency (updates per case) and faithfulness across ablation variants. Each point represents the mean over 40 cases; vertical bars indicate 95\% confidence intervals for faithfulness. V0 represents a balanced trade-off, achieving comparable faithfulness to higher-frequency baselines with substantially fewer updates. Lower update frequency and higher faithfulness are preferred.}
\label{fig:efficiency-frontier}
\end{figure}


While EXAIM generates more updates, these updates contain higher informational value and significantly less redundancy. Turn-based summarization compresses multi-topic agent turns into single summaries, leading to information loss and reduced grounding.

\subsubsection{Impact of Semantic Buffering}

Removing the semantic buffering layer (V2) results in a dramatic increase in update frequency (46.4 vs. 11.7 updates per case) and output volume (4312 vs. 1390 CTU). Without semantic filtering, the system produces excessive, fragmented updates, increasing update frequency and output volume and lowering semantic yield per update. This confirms that syntax-aware segmentation alone is insufficient for real-time explainability.

\subsubsection{Effect of Novelty Filtering}

Disabling novelty filtering (V4) increases update frequency (16.5 vs. 11.7 updates per case) without materially improving redundancy reduction (0.346 vs. 0.366). Although V4 captures marginally more information (coverage 0.175 vs. 0.162), it does so at the cost of substantially higher interruption frequency. This demonstrates that novelty filtering functions as an update frequency control mechanism rather than a simple deduplication strategy.

\subsubsection{Adaptive vs. Fixed Segmentation}

Replacing syntax-aware segmentation with fixed-size chunking (V3) degrades both coverage (0.134 vs. 0.162) and faithfulness (0.382 vs. 0.421). Fixed segmentation frequently splits semantically complete reasoning units, forcing the summarizer to omit or infer missing context. Adaptive segmentation preserves logical coherence and improves grounding.

\subsubsection{Computational Overhead}

EXAIM introduces additional computational cost due to semantic analysis of the reasoning stream. Average summary latency increases from 1.03s (V1) to 1.28s (V0), with p95 tail latencies of 1.59s (V1) and 1.84s (V0). In streaming systems, tail latencies are critical for user experience; the observed p95 values remain within acceptable bounds for interactive dashboard update latencies. Relative token usage increases by 5.3$\times$. While token usage is higher, this overhead reflects the cost of semantic analysis required for redundancy suppression and improved contract-groundedness and coverage.

\section{Discussion}

The results of this study highlight a fundamental mismatch between existing interaction abstractions and the operational realities of multi-agent clinical AI systems. In particular, they demonstrate that turn-based interfaces—while effective for human dialogue—are ill-suited for mediating transparency in distributed LLM reasoning environments. EXAIM's performance gains stem not from exposing more information, but from restructuring how and when information is surfaced.

\subsection{Rethinking Turns as a Transparency Primitive}

A central insight from the evaluation is that agent turns do not reliably correspond to semantic progression in multi-agent reasoning. Turn-based summarization compresses heterogeneous content into single updates, leading to reduced coverage and weaker grounding. Conversely, EXAIM's event-driven approach decouples explanation timing from generation mechanics, allowing clinically meaningful updates to be surfaced at sub-turn granularity. This finding suggests that explainability mechanisms should treat reasoning streams as continuous processes rather than discrete conversational exchanges. For system designers, this implies that transparency should be mediated by semantic state changes rather than orchestration artifacts.

\subsection{Semantic Buffering as Update Stream Control}

The ablation results demonstrate that semantic buffering is essential for balancing transparency and update frequency. Without semantic filtering, the system produces frequent, low-value updates that reduce information density; without novelty gating, it surfaces marginal informational gains at the cost of excessive interruption. EXAIM's buffering strategy effectively converts high-frequency, verbose reasoning streams into sparse, high-signal clinician-facing updates. This behavior positions EXAIM not as an explanation generator, but as an update stream control layer—one that regulates information flow through redundancy suppression and update frequency regularization. Such control is particularly important in high-stakes environments, where excessive transparency can be as operationally undesirable as insufficient transparency.

\subsection{Trade-offs Between Coverage and Update Frequency}

The evaluation reveals an inherent trade-off between maximal information coverage and interruption rate/output volume. Variants without novelty filtering achieve slightly higher coverage by surfacing incremental updates, but at the cost of significantly higher interruption frequency. EXAIM explicitly prioritizes stream efficiency by suppressing low-impact updates, accepting a small reduction in total coverage to regularize update frequency. This trade-off reflects a broader principle in clinical system design: explainability must be selective rather than exhaustive. Transparency that increases update frequency and redundancy undermines its own operational value.

\subsection{Implications for Clinical AI System Design}

The findings suggest several implications for future clinical AI systems:
\begin{itemize}
\item Explainability should be treated as an independent architectural concern, separable from diagnostic reasoning.
\item Middleware layers provide a natural locus for managing transparency in complex, multi-agent systems.
\item Structured, schema-constrained explanations better support clinical sensemaking than free-form reasoning traces.
\item Incremental, event-driven disclosure aligns more closely with real-world clinical workflows than post-hoc summaries.
\end{itemize}

Collectively, these insights support a shift from explanation-as-output toward explanation-as-process in clinical AI design.

\section{Conclusion}

Multi-agent large language model systems offer powerful capabilities for clinical decision support, but their adoption is constrained by the difficulty of interpreting verbose, distributed reasoning processes in real time. Existing explainability approaches largely operate post hoc or expose raw reasoning traces without regard for update frequency and redundancy suppression, limiting their utility in time-sensitive clinical environments.

This paper introduced EXAIM, a real-time middleware architecture for process-level explainability in multi-agent clinical decision support systems. By decoupling transparency from diagnostic reasoning, EXAIM regulates streaming agent outputs through semantic event detection and schema-constrained summarization. The architecture enables incremental, clinician-aligned updates that preserve attribution and uncertainty while suppressing redundant or low-value information.

Through a controlled ablation study on multi-agent clinical reasoning traces, we demonstrated that semantic buffering significantly improves information coverage and faithfulness while reducing redundancy compared to turn-based and heuristic baselines. These gains are achieved with modest computational overhead, highlighting the feasibility of middleware-level explainability for real-world deployment.

More broadly, this work reframes explainability in clinical AI as a problem of information mediation rather than explanation exposure. As multi-agent systems become increasingly prevalent in high-stakes domains, architectures like EXAIM illustrate how real-time transparency can be achieved without inducing high-frequency, low-yield update streams or constraining upstream reasoning. Future clinical AI systems can build on this approach to deliver explainability that is not only faithful, but operationally effective.

\subsection{Limitations}

This study has several limitations that warrant consideration. First, the evaluation relies on replayed reasoning traces rather than live clinical deployment. While this approach enables controlled, reproducible comparison across variants, it does not capture downstream effects on clinician behavior, trust, or decision quality. Future work should incorporate human-in-the-loop evaluations to assess real-world impact.

Second, the dataset consists of rare-disease diagnostic cases, which tend to produce dense and exploratory reasoning traces. Performance characteristics may differ in routine clinical scenarios with more stable diagnostic trajectories. Further evaluation across diverse case types is needed to assess generalizability.

Third, semantic event detection in EXAIM relies on LLM-based analysis, which introduces additional computational cost. While the observed latency remains acceptable for reading tasks, future work could explore lightweight classifiers or distilled models to reduce overhead without sacrificing semantic sensitivity.

Finally, the evaluation employs automated proxy metrics to assess coverage, redundancy, and faithfulness. As detailed in Section V-A.5, concept-level faithfulness metrics operate at the level of explicit lexical realization rather than semantic equivalence, systematically under-scoring compressed summaries due to abbreviation, normalization, compression, and schema remapping. While relative comparisons across variants remain informative, these metrics cannot fully capture clinical nuance, contextual relevance, or semantic entailment. Complementary qualitative evaluation—including clinician-judged factual consistency, sentence-level entailment verification, and task-based assessment—will be necessary to validate the practical utility and safety of the system.

\section*{Acknowledgment}

The authors would like to thank the reviewers for their valuable feedback and suggestions.

\begin{thebibliography}{00}
\bibitem{b1} R. T. Sutton, D. Pincock, D. C. Baumgart, D. C. Sadowski, R. N. Fedorak, and K. I. Kroeker, ``An overview of clinical decision support systems: benefits, risks, and strategies for success,'' \textit{npj Digital Medicine}, vol. 3, no. 1, p. 17, 2020.

\bibitem{b2} Y. Miyachi, O. Ishii, and K. Torigoe, ``Design, implementation, and evaluation of the computer-aided clinical decision support system based on learning-to-rank: collaboration between physicians and machine learning in the differential diagnosis process,'' \textit{BMC Medical Informatics and Decision Making}, vol. 23, no. 1, p. 26, 2023.

\bibitem{b3} X. Chen et al., ``Enhancing diagnostic capability with multi-agents conversational large language models,'' \textit{npj Digital Medicine}, 2025. [Online]. Available: https://doi.org/10.1038/s41746-025-01550-0

\bibitem{b4} Y.-J. Chen, A. Albarqawi, and C.-S. Chen, ``Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance,'' \textit{arXiv preprint arXiv:2501.XXXXX}, 2025.

\bibitem{b5} Q. Peng, J. Cui, J. Xie, Y. Cai, and Q. Li, ``Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree,'' in \textit{Proc. 33rd ACM Int. Conf. Multimedia}, 2025.

\bibitem{b6} Q. Abbas, W. Jeong, and S. W. Lee, ``Explainable AI in Clinical Decision Support Systems: A Meta-Analysis of Methods, Applications, and Usability Challenges,'' \textit{Healthcare}, vol. 13, no. 17, p. 2154, 2025.

\bibitem{b7} Z. Ben-Zion et al., ``Assessing and alleviating state anxiety in large language models,'' \textit{npj Digital Medicine}, vol. 8, no. 1, p. 132, 2025.

\bibitem{b8} M. Salimiparsa, D. Lizotte, and K. Sedig, ``A User-Centered Design of Explainable AI for Clinical Decision Support,'' in \textit{Canadian AI}, 2021.

\bibitem{b9} A. Silva, M. Schrum, E. Hedlund-Botti, N. Gopalan, and M. Gombolay, ``Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction,'' \textit{Int. J. Human-Computer Interaction}, 2023.

\bibitem{b10} M. Sanwal, ``Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models,'' \textit{arXiv preprint arXiv:2501.09967}, 2025.

\bibitem{b11} A. Awais Sani and A. Wahab, ``Towards More Explainable AI: Layered Chain-of-Thought Prompting in Multi-Agent Systems,'' \textit{ResearchGate}, 2024.

\bibitem{b12} C. Derksen, F. M. Walter, A. B. Akbar, A. V. E. Parmar, T. S. Saunders, T. Round, G. Rubin, and S. E. Scott, ``The implementation challenge of computerised clinical decision support systems for the detection of disease in primary care: systematic review and recommendations,'' \textit{Implementation Science}, vol. 20, no. 1, p. 33, 2025.

\bibitem{b13} R. Marcilly, E. Ammenwerth, E. Roehrer, J. Niès, and M.-C. Beuscart-Zéphir, ``Evidence-based usability design principles for medication alerting systems,'' \textit{BMC Medical Informatics and Decision Making}, vol. 18, no. 1, p. 69, 2018.

\bibitem{b14} K. Le-Duc, K.-N. Nguyen, L. Vo-Dang, and T.-S. Hy, ``Real-time Speech Summarization for Medical Conversations,'' in \textit{Proc. Interspeech}, 2025, pp. 1960--1964.

\bibitem{b15} Y. Wu, C. Zhao, Y. Cao, X. Xu, Y. Mehdad, M. Ji, and C. N. Cheng, ``Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback,'' \textit{arXiv preprint arXiv:2510.06677}, 2025.

\bibitem{b16} S. Hong, L. Xiao, X. Zhang, and J. Chen, ``ArgMed-Agents: Explainable Clinical Decision Reasoning with LLM Discussion via Argumentation Schemes,'' \textit{arXiv preprint arXiv:2403.06294}, 2024.

\bibitem{b17} M. C. Ozgun, J. Pei, K. Hindriks, L. Donatelli, Q. Liu, and J. Wang, ``Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis,'' in \textit{Proc. 34th ACM Int. Conf. Information and Knowledge Management}, 2025.

\bibitem{b18} D. Van Veen et al., ``Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts,'' \textit{Nature Medicine}, vol. 30, no. 4, pp. 1134--1142, 2023.

\bibitem{b19} K. Krishna, S. Khosla, J. Bigham, and Z. C. Lipton, ``Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques,'' in \textit{Proc. 59th Annu. Meeting Assoc. Computational Linguistics}, 2021, pp. 4958--4972.

\bibitem{b20} L. Zhang, C. Hart, S. Burger, and T. Schaaf, ``Annotate the Way You Think: An Incremental Note Generation Framework for the Summarization of Medical Conversations,'' in \textit{Proc. LREC-COLING}, 2024, pp. 1173--1181.

\bibitem{b21} A. Bailly, A. Saubin, G. Kocevar, and J. Bodin, ``Divide and summarize: improve SLM text summarization,'' \textit{Frontiers in Artificial Intelligence}, vol. 8, p. 1604034, 2025.

\bibitem{b22} F. L. Piya and R. Beheshti, ``ConTextual: Improving Clinical Text Summarization in LLMs with Context-Preserving Token Filtering and Knowledge Graphs,'' \textit{arXiv preprint}, 2025.

\bibitem{b23} F. Schneider, M. Turchi, and A. Waibel, ``Policies and Evaluation for Online Meeting Summarization,'' in \textit{Interspeech}, 2024.

\bibitem{b24} A. Ehtesham, A. Singh, and S. Kumar, ``Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework,'' \textit{arXiv preprint arXiv:2506.13800}, 2025.

\bibitem{b25} K. Peffers, T. Tuunanen, M. Rothenberger, and S. Chatterjee, ``A design science research methodology for information systems research,'' \textit{J. Management Information Systems}, vol. 24, no. 3, pp. 45--77, 2007.

\end{thebibliography}

\end{document}

