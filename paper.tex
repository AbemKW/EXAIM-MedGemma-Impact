\documentclass[conference]{IEEEtran}

\IEEEoverridecommandlockouts

% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{EXAIM: Explainable AI Middleware for Real-Time Multi-Agent Clinical Decision Support}

\author{\IEEEauthorblockN{1\textsuperscript{st} Author Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email@example.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Author Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email@example.com}
}

\maketitle

\begin{abstract}
Clinical decision support systems (CDSS) increasingly leverage multi-agent large language model (LLM) architectures to decompose complex diagnostic reasoning into specialized components. However, these systems generate verbose, interleaved reasoning traces that are difficult for clinicians to interpret in real time, limiting transparency and clinical adoption. We present EXAIM (Explainable AI Middleware), a novel middleware architecture that transforms live multi-agent reasoning traces into structured, concise summaries optimized for clinical comprehension. EXAIM employs a three-layer pipeline: (1) TokenGate, a syntax-aware pre-buffer that regulates token flow using configurable word thresholds and boundary detection; (2) BufferAgent, an LLM-powered semantic boundary detector that uses structured output analysis to trigger summarization based on completeness, relevance, novelty, and topic coherence; and (3) SummarizerAgent, a schema-constrained compressor that generates summaries aligned with clinical communication frameworks (SBAR/SOAP) while enforcing evidence-based character limits. Unlike post-hoc explanation methods, EXAIM operates incrementally on live reasoning streams, reducing cognitive load while preserving diagnostic transparency. Our architecture addresses critical gaps in CDSS explainability by providing process-level narrative transparency rather than feature-importance visualizations, supporting real-time clinical workflows, and maintaining multi-agent attribution. The system's modular design enables integration with existing multi-agent CDSS frameworks while remaining agnostic to specific clinical domains or agent implementations. This work contributes a reproducible, evidence-grounded approach to explainable multi-agent clinical AI, with implications for human-AI collaboration in time-sensitive medical decision-making.
\end{abstract}

\begin{IEEEkeywords}
Clinical decision support systems, explainable artificial intelligence, multi-agent systems, large language models, real-time summarization, middleware architecture, human-AI collaboration
\end{IEEEkeywords}

\section{Introduction}

Clinical decision support systems (CDSS) increasingly employ multi-agent large language model (LLM) architectures to decompose complex diagnostic reasoning into specialized components such as information retrieval, differential diagnosis, uncertainty estimation, and safety review. While these architectures improve diagnostic performance and robustness, they introduce a critical usability challenge: multi-agent systems generate long, interleaved reasoning traces that are difficult for clinicians to interpret during time-sensitive decision-making.

Existing approaches to explainability in clinical AI predominantly rely on post-hoc methods such as feature importance visualizations, attention maps, or static textual explanations generated after inference completion. Although these are valuable for auditing model behavior, these methods are poorly aligned with real-time clinical workflows. They expose internal model mechanics rather than the evolving clinical narrative. Traditional explainability approaches fail to account for distributed reasoning and cognitive overload with verbose or redundant information.

Conversational interfaces for multi-agent systems typically rely on turn-based interaction paradigms, surfacing outputs only at agent turn boundaries. In human-human communication, turn-taking corresponds to semantic completion; however, in multi-agent LLM systems, a single turn may contain multiple topic shifts, partial hypotheses, or exploratory reasoning. Treating turns as semantic units leads to lossy compression, redundancy, and reduced faithfulness when reasoning traces are summarized for clinical consumption.

These limitations highlight the need for a real-time, process-level explainability mechanism that operates during reasoning rather than after it. Such a mechanism must regulate information flow, identify semantically meaningful update points, and present structured summaries that align with clinical communication practices while preserving attribution and uncertainty.

This study introduces an explainable AI middleware (EXAIM), a real-time summarization middleware architecture designed to bridge the gap between multi-agent reasoning complexity and clinical information needs. EXAIM operates as an intermediary layer between upstream diagnostic agents and clinician-facing interfaces, continuously monitoring reasoning streams and generating concise, structured summaries at clinically meaningful moments. Rather than relying on fixed turn boundaries or static thresholds, EXAIM employs semantic event detection to trigger updates only when new, relevant, and non-redundant clinical information emerges.

EXAIM is implemented as a modular, domain-agnostic middleware composed of three components: (i) a syntax-aware stream regulator that segments high-velocity token streams into semantically coherent units, (ii) a semantic buffering agent that detects topic shifts, novelty, and critical events, and (iii) a schema-constrained summarization agent that produces structured updates aligned with established clinical communication frameworks. The architecture supports incremental transparency, reduces cognitive load, and preserves multi-agent attribution without requiring modification of upstream diagnostic models.

We evaluate EXAIM using replayed multi-agent clinical reasoning traces and compare it against turn-based and heuristic baselines through a controlled ablation study. Results demonstrate that semantic buffering significantly improves trace coverage and faithfulness while reducing redundant updates, at the cost of modest computational overhead. These findings suggest that middleware-level intervention is a viable and effective strategy for enabling real-time explainability in multi-agent clinical AI systems.

This paper makes the following contributions:
\begin{itemize}
\item \textbf{Middleware Architecture for Real-Time Explainability:} We propose EXAIM, a modular middleware architecture that transforms streaming multi-agent reasoning traces into structured, clinician-aligned summaries without modifying upstream diagnostic agents.
\item \textbf{Semantic Event-Driven Summarization:} We introduce a semantic buffering mechanism that detects topic shifts, novelty, and critical clinical events to trigger summaries based on meaning rather than agent turn boundaries.
\item \textbf{Schema-Constrained Clinical Summarization:} We operationalize process-level transparency through structured summaries aligned with SBAR/SOAP-inspired communication patterns, preserving agent attribution and uncertainty while enforcing strict brevity constraints.
\item \textbf{Empirical Evaluation of Streaming Explainability Tradeoffs:} Through a controlled ablation study, we quantify the impact of semantic buffering on coverage, redundancy, faithfulness, and computational cost in multi-agent clinical reasoning streams.
\end{itemize}

\section{Background and Related Work}

\subsection{Clinical Decision Support Systems and Explainability}

Clinical decision support systems (CDSS) have demonstrated potential to improve diagnostic accuracy, guideline adherence, and patient safety, yet their real-world adoption remains constrained by usability and workflow integration challenges. Prior studies emphasize that excessive alerting, poor timing, and opaque system behavior contribute to clinician frustration and alert fatigue, limiting trust and sustained use. As CDSS increasingly incorporate machine learning and large language models (LLMs), explainability has emerged as a critical requirement for safe deployment in clinical environments.

Explainable AI (XAI) approaches in CDSS have largely focused on post-hoc interpretation methods, including feature importance rankings, attention visualization, and surrogate model explanations. While these techniques provide insight into model behavior, they primarily expose internal model mechanics rather than supporting clinical sensemaking. Empirical studies show that clinicians often prefer concise, narrative explanations grounded in clinical reasoning over probabilistic or feature-based explanations, particularly in time-sensitive contexts. This misalignment between existing XAI techniques and clinical information needs motivates alternative approaches to explainability that prioritize process transparency and communication fidelity.

\subsection{Multi-Agent Systems for Clinical Reasoning}

Multi-agent LLM architectures have gained traction as a means of decomposing complex clinical reasoning tasks across specialized agents responsible for information retrieval, hypothesis generation, verification, and safety assessment. Prior work demonstrates that distributing reasoning across agents can improve diagnostic performance, robustness, and oversight compared to monolithic models. These systems also offer theoretical advantages for transparency, as individual agents can be assigned interpretable roles within the diagnostic process.

However, multi-agent systems introduce new explainability challenges. Agent interactions generate long, interleaved reasoning traces that include exploratory hypotheses, internal deliberation, and repetitive grounding statements. While some systems explicitly expose agent reasoning to users, these traces are typically verbose and require post-hoc review, limiting their practical utility during live clinical decision-making. Existing multi-agent frameworks largely assume that transparency is achieved by exposing reasoning in full, rather than by managing how and when information is presented to clinicians.

\subsection{Turn-Based Interfaces and Streaming Reasoning}

Most conversational and multi-agent AI systems adopt turn-based interaction paradigms, surfacing outputs only when an agent completes a turn. This abstraction is inherited from human dialogue systems, where turn-taking often corresponds to semantic completion. In multi-agent LLM systems, however, a single turn may contain multiple topic shifts, partial inferences, or exploratory reasoning branches. Treating turns as semantic units can therefore result in lossy compression, redundancy, and reduced faithfulness when summarization is applied.

Recent work on streaming and incremental reasoning highlights the limitations of turn-based interfaces in high-velocity information environments. Studies in meeting summarization, customer support, and medical dialogue demonstrate that incremental updates triggered by semantic boundaries rather than fixed intervals improve information retention and reduce cognitive load. These findings suggest that real-time systems require adaptive mechanisms to regulate information flow based on content, not generation structure.

\subsection{Clinical Summarization and Structured Communication}

Clinical summarization research provides important foundations for managing information overload in healthcare settings. Prior work demonstrates that structured summaries aligned with clinical documentation standards, such as SBAR and SOAP, improve comprehension, reduce errors, and support handoff communication. Large language models have shown strong performance in generating clinical summaries when explicit length and structure constraints are enforced.

Recent studies further indicate that incremental or streaming summarization can outperform post-hoc summarization by preserving temporal context and reducing the need for retrospective compression. However, most clinical summarization systems operate on completed conversations or documents and do not address the challenges posed by multi-agent reasoning streams. Specifically, they assume a single narrative source rather than multiple interacting agents producing overlapping and partially redundant content.

\subsection{Middleware Architectures for Clinical AI}

Middleware approaches have been proposed to integrate AI reasoning with electronic health records and clinical workflows, enabling data normalization, orchestration, and post-processing. These architectures demonstrate the feasibility of inserting intermediate layers between AI models and user interfaces to manage complexity and ensure compliance. However, existing middleware solutions primarily focus on data access, interoperability, or static summarization and do not address real-time explainability for streaming multi-agent reasoning.

To our knowledge, prior systems do not jointly (i) perform semantic event triggering over interleaved multi-agent traces and (ii) emit clinician-aligned schema summaries as a decoupled middleware layer. EXAIM builds insights from XAI, clinical summarization, and streaming systems by introducing a dedicated middleware layer that regulates information flow, detects meaningful reasoning events, and generates clinician-aligned summaries in real time.

\section{EXAIM Architecture}

EXAIM follows an event-driven architecture composed of three sequential components: (1) a TokenGate for syntax-aware stream regulation, (2) a BufferAgent for semantic event detection, and (3) a SummarizerAgent for schema-constrained synthesis. Each component progressively increases the semantic abstraction of the incoming data while enforcing strictly defined flow-control policies.

\subsection{Middleware Placement and Data Flow}

EXAIM is positioned downstream of all diagnostic agents and upstream of the clinician-facing interface. The system operates on a ``push'' model where agents emit tokens asynchronously to the middleware. Unlike turn-based systems that rely on orchestration artifacts (e.g., turn\_end events), EXAIM intercepts the raw token stream, allowing it to operate at a finer granularity than conventional conversational interfaces. The middleware maintains no global diagnostic authority; instead, it functions as an interpretive layer that monitors reasoning progression and emits summaries only when specific semantic thresholds are crossed.

\subsection{Syntax-Aware Stream Regulation (TokenGate)}

The first stage of the EXAIM pipeline regulates the high-velocity output of large language models into semantically coherent segments suitable for downstream analysis. Raw token streams produced by LLMs are often fragmented, bursty, and misaligned with linguistic boundaries. Processing such streams directly leads to excessive summarization triggers or incomplete semantic units.

To maintain low latency and avoid dependencies on model-specific tokenizers (e.g., BPE, TikToken), TokenGate operates on whitespace-delimited word counts. The TokenGate accumulates tokens into a per-agent buffer and flushes only when specific structural or temporal conditions are met. These flush policies are prioritized to balance latency with linguistic integrity:

\begin{itemize}
\item \textbf{Boundary-Aware Accumulation:} To prevent severing sentences mid-thought, the gate accumulates tokens until a minimum word threshold (min\_words=60) is met and a boundary cue is detected. We employ a regex pattern that targets sentence-terminating punctuation followed by optional closures: \texttt{r"[.? !][ ) \textbackslash ] \textbackslash \}'\ "]* \$"}.
\item \textbf{Silence Detection:} To handle bursty agent behavior, a silence\_timer (1.0s) forces a flush if no new tokens are received, preventing data from becoming ``stuck'' during agent retrieval pauses.
\item \textbf{Safety Valves:} To guarantee upper-bound latency during verbose generation, a hard flush is forced if the buffer exceeds max\_words=100 or a max\_wait\_timeout of 4.0s, regardless of boundary cues.
\end{itemize}

\subsection{Semantic Buffering and Event Detection (BufferAgent)}

The core innovation of EXAIM lies in its semantic buffering layer, which determines when a clinician-facing update should be generated. Rather than triggering summaries at fixed intervals or agent turn boundaries, EXAIM evaluates the semantic content of buffered segments to detect meaningful reasoning events.

This component analyzes incoming segments using a structured BufferAnalysis state machine, classifying the stream into one of three states:
\begin{itemize}
\item \textbf{SAME\_TOPIC\_CONTINUING:} Elaboration on a previous concept
\item \textbf{TOPIC\_SHIFT:} Movement to a new clinical domain or reasoning phase
\item \textbf{CRITICAL\_ALERT:} Immediate life-safety information
\end{itemize}

Beyond state classification, the BufferAgent assesses incoming segments along three dimensions:
\begin{itemize}
\item \textbf{Completeness:} whether the segment represents a finished clinical thought rather than an incomplete clause or speculative fragment.
\item \textbf{Relevance:} whether the content contributes to clinically meaningful information, as opposed to internal agent coordination or procedural reasoning.
\item \textbf{Novelty:} whether the information introduces new diagnostic insight relative to previously surfaced summaries.
\end{itemize}

Based on this semantic assessment, the middleware decides whether to trigger summarization or continue accumulating context. Crucially, EXAIM distinguishes verbosity from progression. Multi-agent systems often restate known facts to ground internal reasoning; such repetition is filtered to avoid redundant clinician notifications. At the same time, EXAIM ensures that clinically significant transitions—such as changes in diagnostic focus or emergent safety concerns—are surfaced promptly.

\subsection{Schema-Constrained Clinical Summarization (SummarizerAgent)}

When a semantic event is detected, the accumulated context is passed to the summarization layer. This component generates a concise, structured update that reflects the current state of multi-agent reasoning rather than a retrospective summary of the entire case. Summaries are constrained to a fixed schema aligned with established clinical communication frameworks (e.g., SBAR/SOAP). Each summary captures: (i) the current diagnostic status or action, (ii) key clinical findings, (iii) rationale for leading hypotheses, (iv) uncertainty or confidence considerations, (v) recommended next steps, and (vi) attribution of contributing agents.

Strict structural and length constraints are enforced to ensure consistency, readability, and cognitive manageability. The summarization layer is explicitly prohibited from inventing or extrapolating information beyond the provided context. If no new information is available for a given field, the summary reflects this absence rather than repeating prior content. By enforcing schema-level constraints, EXAIM produces updates that are predictable in form, comparable over time, and suitable for integration into clinician-facing dashboards or electronic health record workflows.

\section{Design Rationale}

EXAIM is motivated by the observation that existing explainability approaches for clinical AI systems inadequately address the dynamics of real-time, multi-agent reasoning. Rather than optimizing for post-hoc transparency or exhaustive trace exposure, the design of EXAIM prioritizes timely, clinically meaningful communication under cognitive constraints. This section articulates the key design decisions underlying the architecture and their theoretical and practical justification.

\subsection{Middleware-Level Explainability}

A foundational design decision in EXAIM is to implement explainability at the middleware level rather than within individual diagnostic agents. Embedding explainability mechanisms directly into agent prompts or reasoning strategies risks entangling transparency with diagnostic performance, complicating validation and limiting portability across systems. By positioning EXAIM as an intermediary layer, explainability is decoupled from reasoning generation. This separation of concerns enables independent evolution of diagnostic agents and explanation mechanisms, supports integration with heterogeneous multi-agent frameworks, and preserves upstream system behavior. Middleware-level intervention also allows EXAIM to observe the full interaction among agents, rather than isolated reasoning fragments, which is essential for process-level transparency.

\subsection{Event-Driven Rather Than Turn-Based Updates}

Traditional conversational interfaces rely on agent turn boundaries to structure interaction. While effective in human dialogue systems, turn-taking is a poor proxy for semantic progression in multi-agent LLM systems. A single agent turn may contain exploratory reasoning, multiple topic transitions, or internal debate, whereas clinically meaningful changes may occur mid-turn. EXAIM therefore adopts an event-driven update strategy in which summaries are triggered by semantic change rather than generation mechanics. This design aligns updates with clinically salient moments, such as the emergence of new evidence, shifts in diagnostic focus, or safety-critical findings. By decoupling update timing from turn structure, EXAIM avoids lossy compression and reduces redundant notifications.

\subsection{Semantic Filtering Over Heuristic Thresholding}

Many streaming summarization systems rely on fixed thresholds—such as token counts or time intervals—to determine when to summarize. While computationally simple, such heuristics are insensitive to content and often produce fragmented or repetitive updates. EXAIM instead evaluates semantic properties of the reasoning stream, including completeness, relevance, and novelty. This filtering strategy allows the middleware to distinguish between verbosity and progression, suppressing repetitive grounding statements while surfacing genuinely new diagnostic information. Semantic filtering also enables EXAIM to adapt to variability in agent behavior, case complexity, and reasoning style without requiring manual retuning of thresholds for each deployment.

\subsection{Novelty as a Cognitive Load Control Mechanism}

A key design insight in EXAIM is that redundancy reduction alone is insufficient for managing clinician cognitive load. Multi-agent systems may produce a sequence of distinct yet marginally informative updates that are technically non-redundant but operationally distracting. To address this, EXAIM explicitly incorporates novelty as a gating criterion. Novelty is evaluated relative to previously surfaced summaries rather than raw agent output, allowing the system to suppress low-impact incremental updates until sufficient informational value has accumulated. This design trades marginal gains in information coverage for substantial reductions in interruption frequency, aligning with evidence-based principles for alert fatigue mitigation in clinical systems.

\subsection{Evidence-Based Constraints for Cognitive Load}

A critical design decision in EXAIM is the enforcement of strict character limits for each field of the summary schema (e.g., 150 characters for Status, 180 for Findings). These constraints are not arbitrary; they are strictly derived from clinical usability and XAI research to prevent information overload.

\textbf{The ``Elements of Style'' for Interruptive Alerts:} Research on EHR usability emphasizes that alerts must be concise and consistently structured to reduce error rates. Usability guidelines specifically identify ``title brevity'' and ``minimal introductory text'' as key factors in reducing the cognitive burden of interruptive alerts. EXAIM operationalizes this by enforcing hard character caps, preventing the generation of ``cluttered'' interfaces that are known to increase workload (NASA-TLX) and degrade performance on critical tasks like managing abnormal results.

\textbf{Cognitive Limits on Explanation:} In the domain of Explainable AI (XAI), Lage et al. demonstrated that increasing the length and complexity of explanations directly overloads human cognitive abilities, harming the user's ability to mentally simulate model behavior. Furthermore, XAI theory suggests that humans prefer short explanations focusing on 1–2 key causes rather than exhaustive causal chains. By capping the Differential/Rationale field at 210 characters, EXAIM forces the system to align with this preference for brevity, ensuring explanations remain within the ``trust'' boundary for human verification.

\textbf{Snippet-Based Summarization:} While some clinical benchmarks (e.g., ACI-Bench) focus on full-note generation, EXAIM targets the ``live snippet'' paradigm. Van Veen et al. showed that for specific clinical queries, explicit length constraints (e.g., ``15 words or less'') significantly improve clinician preference and conciseness compared to unconstrained generation. Our character limits effectively enforce this ``snippet'' modality, ensuring the middleware functions as a real-time monitor rather than a retrospective documenter.

\subsection{Structured Summarization Aligned with Clinical Communication}

EXAIM enforces a fixed schema aligned with established clinical frameworks (SBAR/SOAP). This structural rigidity is a functional requirement for safety, not just a formatting choice.

\textbf{Consistency Reduces Load:} Usability studies confirm that interfaces with clear information organization result in lower cognitive workload. By mapping agent outputs to a consistent, predictable schema (Situation, Background, Assessment), EXAIM allows clinicians to parse updates using pattern recognition rather than active reading.

\textbf{Simplification for Comprehensibility:} Clinical note simplification research explicitly aims to shorten text to improve human comprehensibility. EXAIM's Key Findings and Recommendation fields serve this simplification role, acting as a real-time filter that strips away the complex, multi-turn ``reasoning scaffolding'' of the agent swarm to present only the actionable clinical signal.

\section{Experiment}

To isolate the architectural contribution of EXAIM, we employ a ``glass-box'' evaluation methodology using deterministic replays of multi-agent reasoning traces. This approach allows us to stress-test the middleware's flow-control capabilities independent of variations in upstream agent behavior.

\subsection{Experimental Setting}

To evaluate the EXAIM middleware, we utilized the Multi-Agent Conversation (MAC) framework (Chen et al., 2025) as the upstream trace generator. MAC is a multi-agent diagnostic system where diverse doctor agents and a supervisor collaborate to solve medical cases. For this study, we instrumented the MAC framework to capture the real-time dynamics of these multi-agent interactions. Specifically, we modified the upstream codebase to log granular stream deltas (chunk-level text outputs) and precise generation timestamps (both absolute and relative to the turn start) during the agent reasoning process. This instrumentation enables us to capture the exact cadence of token generation without altering the agents' underlying prompts, roles, or decision-making logic.

The resulting outputs are stored as frozen replay traces. These traces allow us to simulate a live stream deterministically, feeding identical token sequences and timing delays into the EXAIM middleware across all experimental runs. This decoupling ensures that any measured variance in latency or summarization quality is attributable solely to the EXAIM configuration, rather than stochastic variations in the upstream diagnostic agents.

\subsubsection{EXAIM Implementation Details}

To ensure reproducibility and isolate the middleware's contribution from stochastic model variance, we enforce strict configuration constraints:

\begin{itemize}
\item \textbf{Middleware Model:} Both the BufferAgent (trigger logic) and SummarizerAgent (synthesis) are powered by Gemini 2.5 Flash Lite (Google). This model was selected for its high throughput and native support for large context windows.
\item \textbf{Deterministic Sampling:} All middleware components operate at temperature=0.0 to minimize generation variance.
\item \textbf{Constraint Enforcement:} Schema validation and character-limit truncation are enforced using Pydantic v2 in strict mode. If a summary fails validation, it is automatically rejected and not shown to the clinician (recorded as a failure).
\item \textbf{Metric Extraction:} For semantic evaluation, we utilize scispaCy (en\_core\_sci\_sm) with the UMLS Entity Linker to extract canonical medical concepts (CUIs) from both the raw traces and generated summaries.
\end{itemize}

\subsubsection{Ablation Variants}

To isolate the contribution of individual architectural components, we conduct a controlled ablation study comparing the full EXAIM system (V0) against four structural variants. All variants use the same summarization schema to ensure comparability.

\begin{table}[htbp]
\caption{Ablation Variant Comparison Matrix}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Variant} & \textbf{TokenGate} & \textbf{BufferAgent} & \textbf{Novelty} & \textbf{Fixed Chunking} \\
\hline
V0 (Full EXAIM) & \checkmark & \checkmark & \checkmark & — \\
\hline
V1 (Turn-End Baseline) & — & — & — & — \\
\hline
V2 (No BufferAgent) & \checkmark & — & — & — \\
\hline
V3 (Fixed-Chunk) & — & \checkmark & \checkmark & \checkmark \\
\hline
V4 (No Novelty) & \checkmark & \checkmark & — & — \\
\hline
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\subsubsection{Evaluation Metrics}

We assess system performance using ten ``extractor-consistent'' proxy metrics (M1–M10) grouped into four dimensions:

\begin{itemize}
\item \textbf{Volume and Overhead (M1, M2):} We measure the Update Count and Total Output Volume in Character-Normalized Token Units (CTU) to quantify the reduction in clinician reading burden.
\item \textbf{Redundancy (M3):} We calculate the Redundant Update Rate, defined as the percentage of summaries that share $\geq$ 90\% Jaccard similarity in unique UMLS concepts with their immediate predecessor.
\item \textbf{Global Coverage (M4):} The fraction of unique medical concepts (CUIs) present in the full input trace that are successfully captured in the union of all generated summaries.
\item \textbf{Faithfulness (M6b, M10):} We measure Contract-Groundedness (M6b), the percentage of concepts in a summary supported by the current buffer context, and Schema Compliance (M10), the rate of successful JSON generation.
\end{itemize}

\subsection{Results}

\subsubsection{Full EXAIM vs. Turn-Based Baseline}

Full EXAIM (V0) consistently outperforms the turn-based baseline (V1) across core explainability metrics.

\begin{table}[htbp]
\caption{Performance Comparison: V0 vs. V1}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Metric} & \textbf{V0 (EXAIM)} & \textbf{V1 (Turn-Based)} & \textbf{Relative Change} \\
\hline
Coverage & 0.162 & 0.144 & +12.4\% \\
\hline
Faithfulness & 0.421 & 0.333 & +26.6\% \\
\hline
Redundancy & 0.366 & 0.456 & $-$19.8\% \\
\hline
Updates / Case & 11.7 & 8.5 & +37.6\% \\
\hline
\end{tabular}
\label{tab2}
\end{center}
\end{table}

While EXAIM generates more updates, these updates contain higher informational value and significantly less redundancy. Turn-based summarization compresses multi-topic agent turns into single summaries, leading to information loss and reduced grounding.

\subsubsection{Impact of Semantic Buffering}

Removing the semantic buffering layer (V2) results in a dramatic increase in update frequency and output volume.

\begin{table}[htbp]
\caption{Impact of Semantic Buffering: V0 vs. V2}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{V0} & \textbf{V2} \\
\hline
Updates / Case & 11.7 & 46.4 \\
\hline
Output Volume & 1390 & 4312 \\
\hline
\end{tabular}
\label{tab3}
\end{center}
\end{table}

Without semantic filtering, the system produces excessive, fragmented updates, overwhelming the user. This confirms that syntax-aware segmentation alone is insufficient for real-time explainability.

\subsubsection{Effect of Novelty Filtering}

Disabling novelty filtering (V4) increases update frequency without materially improving redundancy reduction.

\begin{table}[htbp]
\caption{Effect of Novelty Filtering: V0 vs. V4}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{V0} & \textbf{V4} \\
\hline
Updates / Case & 11.7 & 16.5 \\
\hline
Coverage & 0.162 & 0.175 \\
\hline
Redundancy & 0.366 & 0.346 \\
\hline
\end{tabular}
\label{tab4}
\end{center}
\end{table}

Although V4 captures marginally more information, it does so at the cost of substantially higher interruption frequency. This demonstrates that novelty filtering functions as a cognitive load control mechanism rather than a simple deduplication strategy.

\subsubsection{Adaptive vs. Fixed Segmentation}

Replacing syntax-aware segmentation with fixed-size chunking (V3) degrades both coverage and faithfulness.

\begin{table}[htbp]
\caption{Adaptive vs. Fixed Segmentation: V0 vs. V3}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{V0} & \textbf{V3} \\
\hline
Coverage & 0.162 & 0.134 \\
\hline
Faithfulness & 0.421 & 0.382 \\
\hline
\end{tabular}
\label{tab5}
\end{center}
\end{table}

Fixed segmentation frequently splits semantically complete reasoning units, forcing the summarizer to omit or infer missing context. Adaptive segmentation preserves logical coherence and improves grounding.

\subsubsection{Computational Overhead}

EXAIM introduces additional computational cost due to semantic analysis of the reasoning stream.

\begin{table}[htbp]
\caption{Computational Overhead: V0 vs. V1}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Metric} & \textbf{V0} & \textbf{V1} \\
\hline
Avg. Summary Latency & 1.28 s & 1.03 s \\
\hline
Relative Token Usage & 5.3$\times$ & 1.0$\times$ \\
\hline
\end{tabular}
\label{tab6}
\end{center}
\end{table}

The observed latency increase remains within acceptable bounds for clinician-facing reading tasks. While token usage is higher, this overhead reflects the cost of converting raw compute into reduced cognitive burden and improved transparency.

\section{Discussion}

The results of this study highlight a fundamental mismatch between existing interaction abstractions and the operational realities of multi-agent clinical AI systems. In particular, they demonstrate that turn-based interfaces—while effective for human dialogue—are ill-suited for mediating transparency in distributed LLM reasoning environments. EXAIM's performance gains stem not from exposing more information, but from restructuring how and when information is surfaced.

\subsection{Rethinking Turns as a Transparency Primitive}

A central insight from the evaluation is that agent turns do not reliably correspond to semantic progression in multi-agent reasoning. Turn-based summarization compresses heterogeneous content into single updates, leading to reduced coverage and weaker grounding. Conversely, EXAIM's event-driven approach decouples explanation timing from generation mechanics, allowing clinically meaningful updates to be surfaced at sub-turn granularity. This finding suggests that explainability mechanisms should treat reasoning streams as continuous processes rather than discrete conversational exchanges. For system designers, this implies that transparency should be mediated by semantic state changes rather than orchestration artifacts.

\subsection{Semantic Buffering as Cognitive Mediation}

The ablation results demonstrate that semantic buffering is essential for balancing transparency and cognitive load. Without semantic filtering, the system overwhelms users with frequent, low-value updates; without novelty gating, it surfaces marginal informational gains at the cost of excessive interruption. EXAIM's buffering strategy effectively converts high-frequency, verbose reasoning streams into sparse, high-signal clinician-facing updates. This behavior positions EXAIM not as an explanation generator, but as a cognitive mediation layer—one that regulates information flow to align with human attentional constraints. Such mediation is particularly important in safety-critical environments, where excessive transparency can be as harmful as insufficient transparency.

\subsection{Trade-offs Between Coverage and Usability}

The evaluation reveals an inherent trade-off between maximal information coverage and operational usability. Variants without novelty filtering achieve slightly higher coverage by surfacing incremental updates, but at the cost of significantly higher interruption frequency. EXAIM explicitly prioritizes usability by suppressing low-impact updates, accepting a small reduction in total coverage to maintain clinician focus. This trade-off reflects a broader principle in clinical system design: explainability must be selective rather than exhaustive. Transparency that disrupts workflow or induces alert fatigue undermines its own value.

\subsection{Implications for Clinical AI System Design}

The findings suggest several implications for future clinical AI systems:
\begin{itemize}
\item Explainability should be treated as an independent architectural concern, separable from diagnostic reasoning.
\item Middleware layers provide a natural locus for managing transparency in complex, multi-agent systems.
\item Structured, schema-constrained explanations better support clinical sensemaking than free-form reasoning traces.
\item Incremental, event-driven disclosure aligns more closely with real-world clinical workflows than post-hoc summaries.
\end{itemize}

Collectively, these insights support a shift from explanation-as-output toward explanation-as-process in clinical AI design.

\section{Limitations}

This study has several limitations that warrant consideration. First, the evaluation relies on replayed reasoning traces rather than live clinical deployment. While this approach enables controlled, reproducible comparison across variants, it does not capture downstream effects on clinician behavior, trust, or decision quality. Future work should incorporate human-in-the-loop evaluations to assess real-world impact.

Second, the dataset consists of rare-disease diagnostic cases, which tend to produce dense and exploratory reasoning traces. Performance characteristics may differ in routine clinical scenarios with more stable diagnostic trajectories. Further evaluation across diverse case types is needed to assess generalizability.

Third, semantic event detection in EXAIM relies on LLM-based analysis, which introduces additional computational cost. While the observed latency remains acceptable for reading tasks, future work could explore lightweight classifiers or distilled models to reduce overhead without sacrificing semantic sensitivity.

Finally, the evaluation employs automated proxy metrics to assess coverage, redundancy, and faithfulness. Although these metrics are grounded in prior work, they cannot fully capture clinical nuance or contextual relevance. Complementary qualitative evaluation will be necessary to validate the practical utility of the system.

\section{Conclusion}

Multi-agent large language model systems offer powerful capabilities for clinical decision support, but their adoption is constrained by the difficulty of interpreting verbose, distributed reasoning processes in real time. Existing explainability approaches largely operate post hoc or expose raw reasoning traces without regard for cognitive load, limiting their utility in time-sensitive clinical environments.

This paper introduced EXAIM, a real-time middleware architecture for process-level explainability in multi-agent clinical decision support systems. By decoupling transparency from diagnostic reasoning, EXAIM regulates streaming agent outputs through semantic event detection and schema-constrained summarization. The architecture enables incremental, clinician-aligned updates that preserve attribution and uncertainty while suppressing redundant or low-value information.

Through a controlled ablation study on multi-agent clinical reasoning traces, we demonstrated that semantic buffering significantly improves information coverage and faithfulness while reducing redundancy compared to turn-based and heuristic baselines. These gains are achieved with modest computational overhead, highlighting the feasibility of middleware-level explainability for real-world deployment.

More broadly, this work reframes explainability in clinical AI as a problem of information mediation rather than explanation exposure. As multi-agent systems become increasingly prevalent in high-stakes domains, architectures like EXAIM illustrate how real-time transparency can be achieved without overwhelming users or constraining upstream reasoning. Future clinical AI systems can build on this approach to deliver explainability that is not only faithful, but operationally effective.

\section*{Acknowledgment}

The authors would like to thank the reviewers for their valuable feedback and suggestions.

\begin{thebibliography}{00}
\bibitem{b1} Chen et al., ``Multi-Agent Conversation Framework for Clinical Decision Support,'' 2025.

\bibitem{b2} I. Lage et al., ``Human Evaluation of Explanations,'' in \textit{Proc. AAAI}, 2019.

\bibitem{b3} T. van Veen et al., ``Clinical Query-Focused Summarization,'' in \textit{Proc. ACL}, 2023.

\end{thebibliography}

\end{document}

