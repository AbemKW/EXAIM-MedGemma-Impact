\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\graphicspath{{evals/data/metrics/figures/final/}}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array}
\usepackage{booktabs}
\usepackage[most]{tcolorbox}

% Churkin Protocol: Color Palette (Harmonious & Minimal)
\definecolor{churkinpurple}{RGB}{94,45,121}    % #5E2D79 (for State)
\definecolor{statusgreen}{RGB}{34,139,34}      % #228B22 (Green for True - Success)
\definecolor{statusred}{RGB}{255,99,71}       % #FF6347 (Light Red/Tomato for False - Stop/Suppressed)
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{EXAIM: Explainable AI Middleware for Real-Time Multi-Agent Clinical Decision Support}

\author{\IEEEauthorblockN{Abem Woldesenbet}
\IEEEauthorblockA{\textit{The Beacom College of Computer \& Cyber Sciences} \\
\textit{Dakota State University}\\
Madison, SD, USA \\
abem.woldesenbet@trojans.dsu.edu}
\and
\IEEEauthorblockN{Andy Behrens}
\IEEEauthorblockA{\textit{Information Systems, College of Business \& Information Systems} \\
\textit{Dakota State University}\\
Madison, SD, USA \\
andy.behrens@dsu.edu}
}

\maketitle

\begin{abstract}
 Clinical decision support systems (CDSS) employing multi-agent architectures achieve high diagnostic performance but face a critical transparency bottleneck: they generate verbose, interleaved reasoning traces that are difficult for clinicians to interpret in real time. We present EXAIM (Explainable AI Middleware), an IT artifact developed under a design science research paradigm that transforms live reasoning streams into structured, concise summaries. EXAIM addresses the transparency-usability gap through three primitives: TokenGate for syntax-aware stream regulation, BufferAgent for semantic event detection, and SummarizerAgent for schema-constrained synthesis aligned with clinical frameworks (SBAR/SOAP) \cite{haig2006sbar,podder2023soap}. In contrast to post-hoc methods, EXAIM operates incrementally, triggering updates based on information novelty rather than fixed intervals. We evaluate the artifact through systematic ablation, parameter calibration, and trace-level analysis on 40 diagnostic cases, verifying that event-driven summarization improves semantic coverage while significantly reducing redundant updates.
\end{abstract}

\begin{IEEEkeywords}
Clinical decision support systems, explainable artificial intelligence, multi-agent systems, large language models, real-time summarization, middleware architecture, human-AI collaboration
\end{IEEEkeywords}

\section{Introduction}

The integration of multi-agent large language model (LLM) architectures into clinical decision support systems (CDSS) for text-based diagnostic reasoning introduces a critical transparency paradox. While these systems enhance diagnostic accuracy by decomposing reasoning across specialized agents \cite{chenx2025diagnostic,chen2025enhancing}, they generate high-velocity, interleaved reasoning traces that create three fundamental usability barriers: (1) \textbf{Information Overload}, where verbose agent discourse overwhelms clinician cognitive capacity \cite{peng2025tree}; (2) \textbf{Structural Misalignment}, where turn-based output abstractions fail to capture semantic boundaries in streaming reasoning \cite{wu2025incremental}; and (3) \textbf{Interpretability Gaps}, where raw traces obscure the synthesized clinical picture required for rapid decision-making \cite{hong2024argmed,abbas2025xai}.

To address these challenges, we develop EXAIM (Explainable AI Middleware), an IT artifact constructed under a design science research (DSR) paradigm that transforms opaque multi-agent text streams into transparent, clinically actionable summaries.

EXAIM achieves this transformation through three architectural innovations: (1) \textbf{Syntax-Aware Stream Regulation (TokenGate)}, which segments bursty token streams into coherent linguistic units to prevent fragmentation; (2) \textbf{Semantic Event Detection (BufferAgent)}, which filters updates based on completeness, relevance, and novelty to reduce redundant interruptions; and (3) \textbf{Schema-Constrained Synthesis (SummarizerAgent)}, which enforces strict output contracts aligned with SBAR/SOAP communication standards to guarantee consistent interface utility.

The following research question guides the evaluation of this artifact:

\textbf{RQ1:} Can event-driven, schema-constrained summarization improve trace utility by increasing semantic coverage while reducing redundant updates?

We operationalize RQ1 through four sub-questions corresponding to distinct evaluation dimensions:
\begin{itemize}
\item \textbf{RQ1a:} Does EXAIM improve global semantic trace coverage under identical schema and length constraints?
\item \textbf{RQ1b:} Does EXAIM reduce redundant and low-novelty updates through semantic buffering and novelty filtering?
\item \textbf{RQ1c:} Does EXAIM preserve contract-grounded faithfulness when limited continuity across summaries is permitted?
\item \textbf{RQ1d:} What impact does semantic event detection introduce relative to simpler triggers such as turn boundaries or fixed-interval chunking?
\end{itemize}

This paper makes the following contributions:
\begin{itemize}
\item \textbf{We develop a modular middleware architecture} that transforms streaming multi-agent reasoning traces into structured, clinician-aligned summaries, addressing the cognitive load gap in real-time CDSS.
\item \textbf{We introduce a semantic buffering mechanism} that detects topic shifts, novelty, and critical clinical events to trigger summaries based on context, thereby reducing irrelevant interruptions relative to turn-based baselines.
\item \textbf{We demonstrate process-level transparency} through structured summaries aligned with SBAR/SOAP communication patterns, validated to preserve agent attribution and uncertainty while enforcing strict brevity constraints.
\item \textbf{We formalize the impact of semantic buffering} on coverage, redundancy, faithfulness, and computational cost through a systematic ablation study on 40 diagnostic cases.
\end{itemize}

The remainder of this paper is organized as follows. Section II positions EXAIM within the XAI-CDSS and streaming summarization literature and synthesizes design gaps. Section III details the DSR-guided artifact, its three modules, and provides a demonstrative case walkthrough. Section IV describes the trace replay dataset, calibration procedure, and ablation experiments, reporting quantitative results. Section V interprets findings via design implications and deployment considerations. Section VI concludes and outlines future work.

\section{Literature Review}
\subsection{Clinical Decision Support Systems and Explainability}
Clinical decision support systems (CDSS) have demonstrated potential to improve diagnostic accuracy and patient safety \cite{sutton2020cdss,miyachi2023learning}, yet their real-world adoption remains constrained by usability and workflow integration challenges \cite{derksen2025cdss,bayor2025cdss,harrison2022patient}. Prior studies emphasize that excessive alerting, poor timing, and opaque system behavior contribute to clinician frustration and alert fatigue, limiting trust and sustained use \cite{marcilly2018alerts}. As CDSS increasingly incorporate machine learning and large language models (LLMs), explainability has emerged as a critical requirement for safe deployment in clinical environments \cite{abbas2025xai,chen2025enhancing,hong2024argmed,salimiparsa2021design}. A recent systematic review of CDSS design emphasizes that effective systems must balance clinical accuracy with usability, integration, and transparency \cite{bayor2025cdss}.

\subsubsection{Post-Hoc XAI Methods and Their Limitations}
Explainable AI (XAI) approaches in CDSS have largely focused on post-hoc interpretation methods. Feature importance techniques, including LIME and SHAP, generate local explanations by perturbing inputs and measuring output sensitivity \cite{abbas2025xai,salimiparsa2021design}. Gradient-based visualization methods, such as GradCAM, highlight salient input regions influencing predictions \cite{goel2022covid}. Surrogate model approaches approximate complex models with interpretable alternatives \cite{ribeiro2016lime,guidotti2018survey}.

Frameworks for evaluating XAI systems emphasize the importance of both technical performance and user-centered criteria, suggesting that effective explainability requires consideration of information presentation, cognitive load, and task alignment \cite{vilone2021explainable}. However, these post-hoc methods face critical limitations in real-time clinical workflows. They are poorly aligned with time-sensitive decision-making \cite{abbas2025xai,salimiparsa2021design}, exposing internal model mechanics rather than the evolving clinical narrative \cite{benzion2025anxiety,salimiparsa2021design,hong2024argmed}.

\subsubsection{Cognitive Load and Information Overload}
Traditional explainability approaches fail to account for distributed reasoning and high-velocity, redundant trace emissions that increase output volume and cognitive overload in clinicians \cite{sanwal2025layered,hong2024argmed}. Recent work emphasizes that AI systems for general practice must address cognitive load through transparency that supports clinical sensemaking rather than exposing internal model mechanics \cite{hor2025design}. The update frequency in clinician-facing interfaces often results in verbose or redundant information that does not reflect the summarized treatment plan \cite{wu2025incremental}.

\subsubsection{Clinician Preference for Narrative Explanations}
Empirical studies show that clinicians often prefer concise, narrative explanations grounded in clinical reasoning over probabilistic or feature-based explanations in time-sensitive contexts \cite{silva2023xai}. This misalignment between existing XAI techniques and clinical information needs motivates alternative approaches to explainability that prioritize process transparency and clinical understanding \cite{abbas2025xai,schoonderwoerd2021patterns,salimiparsa2021design,hong2024argmed,derksen2025cdss}. While XAI methods address model interpretability challenges, multi-agent architectures introduce additional complexity in distributing and coordinating explainable reasoning across specialized components.

\subsection{Multi-Agent Systems for Clinical Reasoning}
Multi-agent LLM architectures have gained traction as a means of decomposing complex clinical reasoning tasks across specialized agents responsible for information retrieval, hypothesis generation, verification, and safety assessment \cite{hong2024argmed,ozgun2025psychotherapy,chen2025enhancing}. Prior work demonstrates that distributing reasoning across agents can improve diagnostic performance, robustness, and oversight compared to monolithic models \cite{chenx2025diagnostic,chen2025enhancing,hong2024argmed,peng2025tree}. These systems also offer theoretical advantages for transparency, as individual agents can be assigned interpretable roles within the diagnostic process \cite{ozgun2025psychotherapy}. However, multi-agent systems introduce new explainability challenges. Agent interactions generate long, interleaved reasoning traces that include exploratory hypotheses, internal deliberation, and repetitive grounding statements \cite{hong2024argmed}. While some systems explicitly expose agent reasoning to users, these traces are typically verbose and require post-hoc review, limiting their practical utility during live clinical decision-making. Existing multi-agent frameworks largely assume that transparency is achieved by exposing reasoning in full, rather than by managing how and when information is presented to clinicians \cite{wu2025incremental,leduc2025speech,schneider2024meeting}. The verbosity of multi-agent reasoning traces is compounded by interface design choices, particularly turn-based interaction paradigms that misalign with semantic completeness.

\subsection{Turn-Based Interfaces and Streaming Reasoning}

\subsubsection{Limitations of Turn-Based Paradigms}
Most conversational and multi-agent AI systems adopt turn-based interaction paradigms, surfacing outputs only when an agent completes a turn. This abstraction is inherited from human dialogue systems, where turn-taking often corresponds to semantic completion. However, in multi-agent LLM systems, a single turn may contain multiple topic shifts, partial inferences, or exploratory reasoning branches. Treating turns as semantic units can therefore result in lossy compression, redundancy, and reduced faithfulness when summarization is applied. When multi-agent reasoning traces are compressed at turn boundaries, semantically incomplete fragments may be forced into summaries, while complete reasoning spans crossing turn boundaries may be artificially split. This leads to information loss, as partial hypotheses and exploratory statements are either omitted or presented without sufficient context.

\subsubsection{Semantic Boundary Detection in Streaming Systems}
Recent work on streaming and incremental reasoning highlights the limitations of turn-based interfaces in high-velocity information environments \cite{leduc2025speech,wu2025incremental}. Studies in meeting summarization demonstrate that incremental updates triggered by semantic boundaries rather than fixed intervals improve information retention and regularize update frequency \cite{schneider2024meeting}. Similar findings emerge in customer support and medical dialogue systems, where content-based triggering outperforms interval-based approaches. These findings suggest that real-time systems require adaptive mechanisms to regulate information flow based on content semantics rather than generation structure. For multi-agent clinical decision support, this implies that summarization triggers should be driven by semantic completeness, clinical relevance, and novelty rather than arbitrary turn boundaries or fixed time intervals.

Addressing these interface challenges requires not only semantic event detection but also structured presentation formats aligned with clinical communication practices and documentation standards.



\subsection{Clinical Summarization and Structured Communication}
Clinical summarization research provides important foundations for managing information overload in healthcare settings \cite{vanveen2023summarization,leduc2025speech,yim2023acibench,krishna2021soap}.

\subsubsection{Clinical Communication Frameworks}
Structured summaries aligned with clinical documentation standards, such as SBAR (Situation-Background-Assessment-Recommendation) and SOAP (Subjective-Objective-Assessment-Plan), support predictable, scannable presentation formats \cite{haig2006sbar,podder2023soap}. These frameworks reduce communication errors and support clinical handoff processes \cite{krishna2021soap,zhang2024annotate}. The SBAR framework structures information into four components: current situation, relevant background context, clinical assessment, and recommended actions. SOAP notes organize clinical encounters into subjective patient reports, objective measurements, diagnostic assessment, and treatment plans. Both frameworks emphasize brevity, actionability, and role-appropriate information density.

\subsubsection{LLM-Based Clinical Summarization}
Large language models have shown strong performance in generating clinical summaries when explicit length and structure constraints are enforced \cite{vanveen2023summarization}. Recent work on LLM-based clinical dialogue summarization has validated the use of expert clinician evaluation for assessing summary quality, emphasizing faithfulness, completeness, and clinical utility as key evaluation criteria \cite{fraile2025llm}.

\subsubsection{Streaming and Real-Time Summarization}
Recent studies indicate that streaming summarization can outperform post-hoc summarization by preserving context and reducing the need for retrospective compression \cite{bailly2025divide}. Additionally, token filtering strategies have been proposed to reduce redundancy in clinical texts \cite{piya2025contextual}. However, most clinical summarization systems operate on completed conversations or documents and do not address the challenges posed by multi-agent reasoning streams \cite{wu2025incremental,schneider2024meeting}. Specifically, they assume a single narrative source rather than multiple interacting agents producing overlapping and partially redundant content. Human-in-the-loop dialogue systems literature emphasizes the importance of faithfulness and coherence as critical evaluation dimensions for interactive systems \cite{chen2022hitl}. To our knowledge, prior systems do not jointly perform semantic event triggering over interleaved multi-agent traces and emit clinician-aligned schema summaries. EXAIM integrates concepts from XAI, clinical summarization, and streaming systems by introducing a dedicated middleware layer that regulates information flow, detects meaningful reasoning events, and generates clinician-aligned summaries in real time. % Added positioning statement

\section{Artifact Description}

Design Science aims to create and evaluate IT artifacts intended to solve identified organizational problems \cite{hevner2004design}. Consistent with the principles of DSR, we develop EXAIM as a solution-oriented IT artifact designed to address the specific problem of cognitive overload in multi-agent CDSS. This approach aligns with recent applications of DSR for developing AI-based clinical tools that balance technical innovation with practical usability requirements \cite{hor2025design}. Following the process model outlined by Peffers et al. \cite{peffers2007dsr}, this section provides a systematic description of the artifact's architecture, key components, and operational logic. We first present the overall framework, then detail the three primary modules (TokenGate, BufferAgent, SummarizerAgent), and finally demonstrate the system's operation through a case walkthrough.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{dsr/DSR.pdf}
\caption{EXAIM Design Science Research Methodology (DSRM) process model showing the six stages of our research lifecycle and the process iteration feedback loops.}
\label{fig:dsr}
\end{figure*}

\subsection{Overall Framework}

EXAIM functions as a transparency middleware that sits between upstream diagnostic multi-agent systems and downstream clinician-facing interfaces. The core architectural challenge is straightforward: multi-agent systems generate thousands of token-level deltas per diagnostic session, yet clinicians require at most a dozen discrete, structured updates. EXAIM bridges this gap by transforming high-velocity, interleaved token streams into semantically bounded summary events.

As illustrated in Figure~\ref{fig:system-architecture}, the system accepts an asynchronous stream of token deltas $D = \{(a_t, \tau_t)\}_{t=1}^T$, where each delta consists of an agent identifier $a_t$ and a text fragment $\tau_t$. This input model is deliberately agnostic to the underlying agent architecture---EXAIM requires only that agents emit text incrementally, making it compatible with any streaming LLM framework.

The system emits a sequence of structured summary objects $S = \{s_1, s_2, \ldots, s_N\}$, where each $s_i$ conforms to a strict JSON schema mapped to clinical communication standards (SBAR/SOAP). Critically, $N \ll T$, representing a meaningful reduction in event frequency.

Three architectural constraints govern the system's behavior. First, \textbf{non-blocking observation}: middleware processing occurs asynchronously in a ``sidecar'' pattern, ensuring that EXAIM's analysis never adds latency to the upstream diagnostic process. Agents continue reasoning uninterrupted while the middleware silently buffers their output. Second, \textbf{schema compliance}: all outputs must validate against the AgentSummary schema or be discarded. This hard contract guarantees UI stability---downstream interfaces can trust that every event will render correctly regardless of upstream model behavior. Third, \textbf{strict grounding}: summaries are generated solely from buffered trace content and limited history ($k=3$ prior summaries), explicitly prohibiting external knowledge retrieval to prevent hallucination.

These constraints reflect a fundamental design philosophy: EXAIM does not attempt to ``improve'' the diagnostic reasoning but rather to make it transparently observable within human cognitive limits.

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{../raw/systemFigure/system.pdf}
    \caption{EXAIM System Architecture showing the three-stage pipeline: (1) Interleaved Input Stream from multi-agent sources, (2) EXAIM Middleware Layer with TokenGate (syntax-aware buffering), BufferAgent (semantic event detection), and SummarizerAgent (schema-constrained synthesis), and (3) Structured Output in SBAR/SOAP format for clinician-facing interfaces.}
    \label{fig:system-architecture}
\end{figure*}


\subsection{TokenGate: Syntax-Aware Stream Regulation}

Raw LLM token streams arrive in unpredictable bursts. In a typical multi-agent diagnostic session, a single agent might emit individual characters during deliberation, then suddenly flush complete paragraphs when reaching a conclusion. This fragmentation creates a fundamental challenge for semantic analysis: to detect topic shifts or assess novelty, the system requires coherent linguistic units---complete clauses or sentences---but the raw stream provides no such boundaries.

TokenGate addresses this gap by functioning as a stateful accumulation buffer with intelligent flush triggers. Rather than forwarding every delta immediately to the semantic analyzer, it accumulates incoming text until a syntactically meaningful boundary is reached. The component operates on a priority-based policy with four trigger conditions, designed to balance linguistic coherence against latency guarantees:

\begin{enumerate}
\item \textbf{Boundary-Aware Flush}: When the buffer contains at least $w_{\text{min}}$ words AND ends with sentence-terminal punctuation (period, question mark, exclamation), the system flushes immediately. This represents the ideal case---a complete, grammatical unit ready for semantic analysis.

\item \textbf{Hard Limit}: If the buffer exceeds $w_{\text{max}}$ words regardless of punctuation, a forced flush occurs. This prevents pathological memory growth when agents generate long unpunctuated streams (e.g., extended medical histories or literature reviews).

\item \textbf{Silence Timeout}: If the agent goes silent for more than $t_{\text{silence}}$ seconds AND the buffer contains at least $w_{\text{min}}$ words, the system flushes. This captures the common pattern where agents ``pause'' between reasoning steps without emitting explicit punctuation.

\item \textbf{Safety Valve}: If total wait time exceeds $t_{\text{max}}$ seconds since the first delta arrived, the system flushes regardless of buffer state. This guarantees liveness---the system will never deadlock waiting for a boundary that may never arrive.
\end{enumerate}

These parameters ($w_{\text{min}}$, $w_{\text{max}}$, $t_{\text{silence}}$, $t_{\text{max}}$) jointly determine the Pareto frontier between linguistic coherence and real-time responsiveness. Section IV-B describes the systematic calibration procedure used to identify optimal values.

\subsection{BufferAgent: Semantic Event Detection}

If TokenGate solves the syntactic segmentation problem, BufferAgent solves the semantic filtering problem. Not every coherent chunk of agent reasoning deserves to interrupt the clinician. An agent might spend several sentences re-stating background knowledge, concurring with a previous diagnosis, or exploring a hypothesis already ruled out by others. Presenting all such content as discrete updates would recreate the very cognitive overload EXAIM aims to prevent.

BufferAgent acts as the system's cognitive filter, analyzing each chunk from TokenGate to determine whether it merits triggering a summary. The decision process operates in two stages, deliberately separating semantic interpretation (performed by an LLM) from control logic (executed by deterministic code). This separation ensures that while the system leverages language model capabilities for content understanding, the final trigger decision remains auditable and explainable.

When a chunk $C$ arrives, the BufferAgent first invokes an LLM classifier to extract four semantic predicates: \textbf{Complete} (does this chunk form a self-contained reasoning unit?), \textbf{Relevant} (does it address the current diagnostic question?), \textbf{Novel} (does it introduce information not already captured in the summary history?), and \textbf{State} (is this a continuation of the current topic, a topic shift, or a critical alert?).

These predicates then feed into a deterministic decision tree. The system triggers if $\mathit{State} = \text{CRITICAL}$ (urgent findings always surface immediately), OR if all three conditions hold ($\mathit{Complete} \land \mathit{Relevant} \land \mathit{Novel}$), OR if the chunk represents a topic shift with novel relevant content ($\mathit{State} = \text{SHIFT} \land \mathit{Relevant} \land \mathit{Novel}$). Formally:
\[
T = \begin{cases}
\text{True} & \text{if } \mathit{State} = \text{CRITICAL} \\
\text{True} & \text{if } \mathit{Complete} \land \mathit{Relevant} \land \mathit{Novel} \\
\text{True} & \text{if } (\mathit{State} = \text{SHIFT}) \land \mathit{Relevant} \land \mathit{Novel} \\
\text{False} & \text{otherwise}
\end{cases}
\]
This logic encodes a specific hypothesis about clinician information needs: they require immediate notification of critical events, comprehensive updates when a reasoning thread reaches a novel conclusion, and transition markers when the diagnostic focus shifts---but not incremental deliberation or redundant restatements.

\subsection{SummarizerAgent: Schema-Constrained Synthesis}

When BufferAgent determines that a chunk merits clinician attention, SummarizerAgent faces the final transformation challenge: compressing potentially verbose agent reasoning into a rigid, scannable format aligned with clinical communication standards. This component embodies the system's commitment to interface stability---no matter how chaotic or verbose the upstream reasoning, the output must conform to a bounded schema that downstream UIs can reliably render.

The agent receives three inputs: the triggering chunk $C$, the agent context (identifier and role), and a sliding window of the three most recent summaries $H_{k=3}$. This limited history serves dual purposes---it enables the summarizer to avoid repeating recently stated information while providing continuity for multi-turn reasoning threads.

Generation employs a three-attempt validation strategy. First, \textbf{Initial Generation}: the LLM generates structured output using strict Pydantic type definitions with schema enforcement (strict=True) mapped to SBAR/SOAP fields (Table~\ref{tab:schema}). Second, \textbf{Retry with Rewrite}: if character limits are exceeded, the system creates a targeted rewrite prompt identifying the violating fields and their required length reductions, then re-invokes the LLM. Third, \textbf{Fallback Truncation}: if validation still fails after the rewrite attempt, the system applies hard truncation at character boundaries to ensure schema compliance.

This schema-first approach differs fundamentally from unconstrained summarization. Traditional abstractive summarizers optimize for semantic coverage or fluency but provide no length guarantees. EXAIM inverts this priority: the character budgets are non-negotiable constraints derived from UI layout requirements, while semantic coverage is maximized subject to these bounds.

The AgentSummary schema (Table~\ref{tab:schema}) enforces a structured contract between the middleware and clinician-facing interfaces. Each of the six fields maps directly to elements of SBAR (Situation-Background-Assessment-Recommendation) and SOAP (Subjective-Objective-Assessment-Plan) clinical communication frameworks. Character budgets operationalize qualitative usability principles into deterministic constraints, ensuring that the schema design reflects three core principles: (1) \textit{Scannability}: brief field lengths force summarization to salient points, preventing verbose blocks that impair rapid comprehension; (2) \textit{Clinical Alignment}: field semantics mirror established documentation workflows, reducing cognitive overhead for clinician users; and (3) \textit{Bounded Contracts}: hard character limits guarantee stable UI footprint regardless of model verbosity, preventing layout shifts or overflow in dashboard interfaces.

\begin{table*}[htbp]
\caption{EXAIM Summary Schema: Clinical Mapping and Bounded Output Budgets}
\label{tab:schema}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{p{2.5cm}p{2.5cm}p{4cm}p{1.5cm}p{4.5cm}}
\toprule
\textbf{Field} & \textbf{Clinical Map} & \textbf{Role} & \textbf{Budget (Chars)} & \textbf{Design Motivation} \\
\midrule
Status / Action & SBAR: Situation & Alert header; scannable update. & 150 chars & Enforces scannability via brief title heuristic \cite{pourian2025alerts}. \\
Key Findings & SOAP: Obj/Subj & Salient facts; ``live snippet'' paradigm. & 180 chars & Supports $\sim$2 concise snippets aligned with targeted summarization limits \cite{vanveen2023summarization}. Structured presentation addresses transparency barriers identified in patient-centered CDSS deployment \cite{harrison2022patient}. \\
Differential & SOAP: Assessment & Diagnostic interpretation. & 210 chars & Bounds complexity to maintain interpretability \cite{lage2019interpretability}. \\
Uncertainty & SOAP: Assessment & Explicit confidence signal. & 120 chars & Simplified framing for trust calibration \cite{goel2022covid}. \\
Rec. / Plan & SOAP: Plan & Actionable next step. & 180 chars & Action-linked explanations \cite{silva2023xai}. \\
Agent Contrib. & System Meta & Attribution of active agents. & 150 chars & Pipeline transparency patterns \cite{donadello2021sexai}. \\
\bottomrule
\end{tabular}
\end{center}
\footnotesize
Note: Character budgets are hard constraints that enforce a bounded UI contract, ensuring stable interface footprint regardless of model verbosity.
\normalsize
\end{table*}

\subsection{Demonstrative Case Walkthrough}

To demonstrate EXAIM's semantic filtering and redundancy detection mechanisms, we analyze a specific interaction sequence from a multi-agent reasoning trace (Case ID: 3949). This trace was generated using the MAC Framework (described in Section IV-A), which simulates a team of diagnostic agents collaborating on a complex medical case. This example illustrates how the middleware distinguishes between verbose repetition and genuinely novel clinical information.

\textbf{1. Context State (Pre-Condition)} Prior to this segment, doctor0 and doctor1 had already established Spinocerebellar Ataxia (SCA) as the leading diagnosis. The middleware had generated summaries capturing this consensus and the recommended genetic tests (Log ID: summary\_3). The active buffer was empty.

\textbf{2. The Raw Input Stream} doctor2 then enters the discussion. The agent generates a verbose response concurring with the previous agents and re-stating the established diagnosis.

\textit{Raw Trace (doctor2):} ``\textit{\#\#\# Most Likely Diagnosis: Spinocerebellar Ataxia (SCA): I concur with both Doctor0 and Doctor1 that the most likely diagnosis is spinocerebellar ataxia... The gradual onset at age 30 years also aligns with hereditary SCAs... \#\#\# Differential Diagnoses: 1. Multiple Systems Atrophy (MSA): While I agree that MSA typically presents with...}''

\textbf{3. BufferAgent Logic (Internal Rationale)} While the dashboard remained visually static (see Fig.~\ref{fig:suppression-demo}), the middleware was actively analyzing the stream. As visualized in Fig.~\ref{fig:logic-card}, the system detected semantic redundancy through its multi-stage evaluation process. Despite the input's clinical relevance, the novelty check failed, causing the middleware to suppress the update and prevent redundancy.

\begin{figure}[h]
    \centering
    \scalebox{0.75}{%
    \begin{tcolorbox}[
        enhanced,
        colback=white,
        colframe=black,
        boxrule=0.5mm,
        arc=2mm,
        drop shadow,
        left=4mm,
        right=4mm,
        top=2mm,
        bottom=2mm,
        title=\textbf{BufferAgent Status [doctor2]},
        fonttitle=\bfseries\normalsize,
        coltitle=black,
        colbacktitle=white,
        halign title=center,
        fontupper=\normalsize
    ]
        \normalsize
        
        \textbf{Input Stream:}
        \vspace{-1mm}
        \begin{quote}
            \itshape "...I concur with both Doctor0 and Doctor1... adding no new information."
        \end{quote}
        \vspace{1.5mm}
        
        \textbf{Decision Flags:}
        \vspace{0.5mm}
        
        \begin{center}
        \begin{tabular}{@{}c@{}}
            \raisebox{5pt}{State} \\
            \tcbox[colback=churkinpurple!15, colframe=churkinpurple, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.5mm, bottom=0.5mm, fontupper=\normalsize]{\textcolor{churkinpurple}{\texttt{SAME\_TOPIC\_CONTINUING}}}
        \end{tabular}
        \end{center}
        
        \vspace{0.1mm}
        \begin{center}
        \begin{tabular}{@{}c@{\hspace{2mm}}c@{\hspace{2mm}}c@{}}
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Complete} \\[-3pt]
                \tcbox[colback=statusgreen!15, colframe=statusgreen, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusgreen}{\textbf{True}}}
            \end{tabular} &
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Relevant} \\[-3pt]
                \tcbox[colback=statusgreen!15, colframe=statusgreen, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusgreen}{\textbf{True}}}
            \end{tabular} &
            \begin{tabular}{@{}c@{}}
                \raisebox{5pt}{Novel} \\[-3pt]
                \tcbox[colback=statusred!15, colframe=statusred, size=small, boxrule=0.5mm, left=2mm, right=2mm, top=0.2mm, bottom=0.2mm, fontupper=\normalsize]{\textcolor{statusred}{\textbf{False}}}
            \end{tabular} \\
        \end{tabular}
        \end{center}
        \vspace{-2mm}
        \begin{center}
            $\downarrow$
        \end{center}
        \vspace{-2mm}
        \begin{center}
            \tcbox[
                enhanced,
                colback=statusred!20,
                colframe=statusred,
                boxrule=0.5mm,
                size=normal,
                fontupper=\bfseries\normalsize,
                left=3mm,
                right=3mm,
                top=0.5mm,
                bottom=0.5mm
            ]{\textcolor{statusred}{Trigger = False (Suppressed)}}
        \end{center}
        
        \vspace{0.1mm}
        \noindent\rule{\linewidth}{0.5mm}
        \vspace{0.5mm}
        
        \textbf{Rationale:} Doctor2 concurs with SCA based on ataxia/dysarthria/MRI... adding no new information. The content is \textcolor{statusgreen}{relevant} as it details planned actions, but it is \textcolor{statusred}{not novel} as these details were already broadly mentioned in previous summaries.
    \end{tcolorbox}%
    }
    \caption{BufferAgent System Status Card showing the hierarchical decision logic. Three evaluation conditions (\textcolor{statusgreen}{Complete=True}, \textcolor{statusgreen}{Relevant=True}, \textcolor{statusred}{Novel=False}) combine to produce the final result: \textcolor{statusred}{Trigger=False (Suppressed)}, preventing redundant update generation.}
    \label{fig:logic-card}
\end{figure}

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{case_walkthrough/CaseWalkthroughImage.png}
\caption{The EXAIM dashboard during redundancy suppression. The left panel shows the raw, verbose stream from doctor2 concurring with the diagnosis. The right panel remains static (displaying the previous doctor1 update), visually confirming that the middleware successfully filtered the non-novel content.}
\label{fig:suppression-demo}
\end{figure*}

\textbf{4. Outcome} Consequently, EXAIM filtered this update, preventing the generation of a low-value summary. The middleware continued to buffer doctor2's subsequent tokens silently until the agent introduced specific new details regarding ``High-Resolution MRI techniques'' and ``Oligoclonal bands,'' which finally triggered a consolidated, high-density update.

\section{Experiments and Results}

We verify the effectiveness of the EXAIM artifact through a "glass-box" evaluation methodology using deterministic replays of multi-agent reasoning traces. This approach allows us to stress-test the middleware's flow-control capabilities independent of variations in upstream agent behavior.

\textbf{Evaluation Objectives}: We aim to experimentally confirm that:
\begin{enumerate}
    \item EXAIM's syntax-aware segmentation and semantic buffering improve information coverage compared to structural baselines (Validating RQ1a).
    \item The novelty detection mechanism reduces redundant updates without sacrificing essential clinical information (Validating RQ1b).
    \item The schema-constrained synthesis preserves faithfulness despite aggressive summarization (Validating RQ1c).
    \item Component-level ablation demonstrates the necessity of the complete three-primitive architecture (Validating RQ1d).
\end{enumerate}

\subsection{Data / Trace Replay Setup}

We utilized the Multi-Agent Conversation (MAC) framework \cite{chenx2025diagnostic} as the upstream trace generator. MAC is a multi-agent diagnostic system where diverse doctor agents and a supervisor collaborate to solve complex medical cases. We executed MAC with GPT-4o-mini using the authors' default configuration without modification. To capture realistic streaming dynamics, we instrumented MAC via a transparent OpenAI API monkeypatch that logs per-delta emissions and microsecond timestamps without altering MAC's conversation logic, speaker selection, or termination behavior. These logs serve as a frozen replay dataset of 40 rare-disease diagnostic cases from MAC's dataset (seed=42), enabling deterministic simulation of live streams.

\subsection{Calibration Protocol}

To ensure fair evaluation, we executed a rigorous parameter selection process before final testing:
\begin{enumerate}
    \item \textbf{TokenGate Calibration}: We performed a grid search over 625 policy combinations (varying word counts and timeouts) on the replay dataset. The final policy (min\_words=60, max\_word=100, silence=1.0s, safe\_wait=4.0s) was selected via Pareto frontier analysis to jointly optimize latency and linguistic coherence.
    \item \textbf{V3 Fixed-Chunk Calibration}: V3 chunk size was derived from V0 TokenGate regular flushes (excluding end-of-trace and turn-end) using a two-stage median: per-case median CTU, then median across the first 40 cases. This ensures V3 operates at comparable segmentation granularity to V0 while isolating the contribution of adaptive boundaries.
    \item \textbf{Measurement Standardization}: We report all token volumes and costs in \textbf{Character-Normalized Token Units (CTU)}, defined as $\lceil \text{len(text)} / 4 \rceil$, to ensure vendor-agnostic reproducibility.
\end{enumerate}
Table~\ref{tab:tokengate-grid} details the explored parameter space.

\begin{table}[htbp]
\caption{TokenGate Parameter Calibration Grid}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lc}
\toprule
\textbf{Parameter} & \textbf{Values Tested} \\
\midrule
min\_words & 30, 40, 50, 60, 70 \\
max\_words & 80, 100, 120, 140, 160 \\
silence\_timer (seconds) & 1.0, 1.5, 2.0, 2.5, 3.0 \\
max\_wait\_timeout (seconds) & 4.0, 5.0, 6.0, 7.0, 8.0 \\
\bottomrule
\end{tabular}
\label{tab:tokengate-grid}
\end{center}
\end{table}

\subsection{Ablation and Baselines}

We isolate the contribution of each architectural primitive through a controlled ablation study. Table~\ref{tab:ablation-variants} defines the five experimental configurations (V0-V4), designed to test specific hypotheses:
\begin{itemize}
    \item \textbf{V0 (Full EXAIM)} includes all primitives.
    \item \textbf{V1 (Turn-End)} removes TokenGate and BufferAgent, triggering only on agent turn ends (testing the "structural vs. semantic" hypothesis).
    \item \textbf{V2 (No BufferAgent)} removes semantic filtering (testing the necessity of the cognitive filter).
    \item \textbf{V3 (Fixed-Chunk)} replaces TokenGate with fixed intervals (testing the value of syntax-awareness).
    \item \textbf{V4 (No Novelty)} disables the novelty check (testing the redundancy suppression mechanism).
\end{itemize}

\begin{table*}[htbp]
\caption{Ablation Variant Configurations}
\label{tab:ablation-variants}
\begin{center}
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccl}
\toprule
\textbf{Variant} & \textbf{TokenGate} & \textbf{BufferAgent} & \textbf{Novelty} & \textbf{Trigger} & \textbf{Description} \\
\midrule
V0 (Full EXAIM) & \checkmark & \checkmark & \checkmark & Semantic & Complete pipeline with all components \\
V1 (Turn-End) & $\times$ & $\times$ & N/A & Turn boundaries & Baseline: summarize only at turn\_end events \\
V2 (No BufferAgent) & \checkmark & $\times$ & N/A & All TokenGate flushes & Tests impact of semantic filtering \\
V3 (Fixed-Chunk) & $\times$ & \checkmark & \checkmark & Fixed CTU intervals & Calibrated fixed-size chunking \\
& (fixed CTU) & & & & vs. adaptive segmentation \\
V4 (No Novelty) & \checkmark & \checkmark & $\times$ & Semantic & Tests novelty gating impact \\
\bottomrule
\end{tabular}
\end{center}
\footnotesize
Note: All variants use identical SummarizerAgent schema and history\_k=3. Checkmark (\checkmark) indicates component enabled; $\times$ indicates disabled. ``N/A'' indicates novelty check not applicable when BufferAgent is disabled.
\normalsize
\end{table*}

\subsection{Metrics}

We prioritized five metrics that directly quantify utility and reliability proxies:

\begin{itemize}
    \item \textbf{Strict Faithfulness (M6b):} The fraction of summary concepts grounded in the full trace:
    \begin{equation}
    M6b = \frac{|\text{summary\_CUIs} \cap \text{trace\_CUIs}|}{|\text{summary\_CUIs}|}
    \end{equation}
    Higher scores indicate stronger concept-level grounding. This metric penalizes unsupported insertions but does not account for semantic paraphrase (see Section IV-C.3). Faithfulness is a standard evaluation criterion in human-in-the-loop dialogue systems \cite{chen2022hitl}.
    \item \textbf{Redundancy Reduction (M3):} Measured via mean Jaccard similarity between consecutive summary updates:
    \begin{equation}
    M3 = \frac{1}{n-1}\sum_{i=1}^{n-1} \frac{|C_i \cap C_{i+1}|}{|C_i \cup C_{i+1}|}
    \end{equation}
    where $C_i$ and $C_{i+1}$ are the sets of unique UMLS concepts (CUIs) extracted from summaries $S_i$ and $S_{i+1}$, and $n$ is the total number of summaries. Lower scores indicate successful suppression of repetitive content.
    \item \textbf{Trace Coverage (M4):} The fraction of unique trace CUIs captured across all summaries:
    \begin{equation}
    M4 = \frac{|\bigcup_{i=1}^{n} \text{summary\_CUIs}_i \cap \text{trace\_CUIs}|}{|\text{trace\_CUIs}|}
    \end{equation}
    This metric is \emph{not normalized by update count}, and therefore inherently favors high-frequency systems. M7 (Budget Efficiency) provides the normalized coverage view.
    \item \textbf{System Latency (M8):} The end-to-end processing time (buffer analysis + summarization generation) to validate the ``real-time'' architectural claim. We report mean, p50 (median), and p95 percentiles across all summary events, as tail latencies are critical for streaming system performance.
    \item \textbf{Schema Compliance (M10):} The rate of successful adherence to the JSON clinical output schema, verifying the system's structural reliability.
\end{itemize}
Supplementary metrics (Update Count, Token Volume, LLM Cost) provide context for trade-off analysis.

\subsubsection{Limitations of Concept-Level Faithfulness Metrics}

Our faithfulness metrics (M6a, M6b) operate at the level of explicit concept realization rather than semantic equivalence or entailment. This introduces a fundamental mismatch: overlap-based metrics like ROUGE fail to correlate with clinical quality when models use paraphrasing rather than literal repetition \cite{fraile2025llm,bailly2025divide}.

Schema-constrained summarization necessarily introduces transformations that reduce recall under strict NER-based concept matching: abbreviation of clinical phrases (e.g., ``acute renal failure'' $\rightarrow$ ``AKI''), normalization of hedged statements and compression omitting modifiers to meet character limits \cite{yim2023acibench}, and schema remapping reorganizing facts into SBAR/SOAP slots through incremental UPDATE and REMOVE operations \cite{zhang2024annotate}.

UMLS-based NER assumes explicit lexical realization and near-literal mention of entities \cite{vanveen2023summarization,yim2023acibench}, misaligning with summaries that preserve semantic content through paraphrase. String-matching approaches demonstrate low correlation with physician preferences when content is normalized or compressed \cite{yim2023acibench,vanveen2023summarization}, causing schema-constrained summarizers to under-score even when accurately representing clinical content \cite{fraile2025llm}.

Despite these limitations, concept-level metrics remain informative for relative comparisons across ablation variants under identical extraction assumptions. Future work should incorporate clinician-judged factual consistency \cite{fraile2025llm}, entailment verification, and task-based evaluation to complement automated assessment.

\subsection{Results}

Table~\ref{tab:primary-metrics} and Table~\ref{tab:supplementary-metrics} present the comparative results.

\begin{table}[htbp]
\caption{Primary Performance Metrics: V0 vs. Baselines}
\begin{center}
\resizebox{\columnwidth}{!}{%
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{V0} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{V4} \\
\midrule
Faithfulness (M6b) & 0.421 & 0.333 & 0.409 & 0.382 & 0.424 \\
Redundancy (M3) & 0.366 & 0.456 & 0.348 & 0.413 & 0.346 \\
Trace Coverage (M4) & 0.162 & 0.144 & 0.312 & 0.134 & 0.175 \\
Latency (M8, s) & & & & & \\
\quad Mean & 1.28 & 1.03 & 1.14 & 1.07 & 1.37 \\
\quad p50 & 1.22 & 0.96 & 1.06 & 1.04 & 1.33 \\
\quad p95 & 1.84 & 1.59 & 1.91 & 1.52 & 2.12 \\
Schema Compliance (M10) & 0.968 & 0.968 & 0.975 & 0.958 & 0.950 \\
\bottomrule
\end{tabular}
}
\label{tab:primary-metrics}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Supplementary Contextual Metrics}
\begin{center}
\resizebox{\columnwidth}{!}{%
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccccc}
\toprule
\textbf{Metric} & \textbf{V0} & \textbf{V1} & \textbf{V2} & \textbf{V3} & \textbf{V4} \\
\midrule
Update Count (M1) & 11.65 & 8.5 & 46.4 & 9.5 & 16.45 \\
Output Volume (M2, CTU) & 1391 & 1148 & 4313 & 1156 & 1763 \\
Unsupported Fraction (M5) & 0.619 & 0.604 & 0.615 & 0.617 & 0.623 \\
Window-Groundedness (M6a) & 0.825 & 0.787 & 0.879 & 0.823 & 0.845 \\
Coverage @ 2000 CTU (M7) & 0.160 & 0.144 & 0.207 & 0.132 & 0.166 \\
LLM Usage (M9, CTU) & 62178 & 11618 & 37151 & 81694 & 58605 \\
\bottomrule
\end{tabular}%
}
\label{tab:supplementary-metrics}
\end{center}
\end{table}

\subsection{Verification Summary}

The experimental results definitively confirm our core hypotheses:
\begin{itemize}
    \item \textbf{Verification of RQ1a (Improvement over Structure)}: V0 outperforms V1 (Turn-End) by 26\% in faithfulness and 20\% in redundancy reduction, while maintaining comparable coverage. This confirms that semantic boundaries offer superior segmentation compared to structural turn boundaries.
    \item \textbf{Verification of RQ1b (Redundancy Suppression)}: Comparing V0 to V4 (No Novelty) shows that novelty filtering is essential for suppressing low-value updates. V4 generates 41\% more updates for only marginal coverage gain, validating the necessity of the novelty primitive.
    \item \textbf{Verification of RQ1c (Faithfulness)}: V0 achieves significantly higher faithfulness (0.421) compared to the disjointed baseline V1 (0.333), confirming that syntax-aware buffering produces more groundable content for the summarizer.
    \item \textbf{Verification of RQ1d (Primitive Necessity)}: The failure of partial variants—V2's extreme volume and V3's poor coverage—demonstrates that BufferAgent and TokenGate are not optional optimizations but necessary components for a viable solution.
\end{itemize}

Taken together, these results demonstrate that EXAIM successfully trades computational cost for a clinically usable, high-signal information stream, solving the transparency paradox identified in the introduction.



\section{Discussion}

Our findings effectively "close the loop" on the research questions, confirming that DSR-guided middleware can mediate the transparency-efficiency trade-off in streaming AI. We structure the implications of this work into theoretical contributions to design science and practical guidance for clinical deployment.

\subsection{Theoretical Implications: Extremes as Design Anchors}

This study extends Design Science Research (DSR) in clinical AI by formalizing the "transparency paradox" as a manageable control problem. 
Historically, CDSS design has oscillated between two extremes: opaque black boxes (high efficiency, low trust) and "firehose" transparency (high trust, cognitive failure). Our ablation results (V0 vs V1/V2) introduce a third stable state: \textbf{Semantic State Transparency}.
By shifting the unit of explanation from the \textit{architectural turn} to the \textit{clinical semantic event}, EXAIM demonstrates that transparency is not a fixed attribute of the model but a tunable variable of the interface. This provides a theoretical design anchor for future multi-agent interfaces: explainability should be decoupled from generation mechanics and governed by an independent logic of clinical meaningfulness.

\subsection{Practical Implications for Workflow Integration}

For practitioners, the success of the BufferAgent logic (V4 vs V0) provides a concrete blueprint for reducing alert fatigue. "Novelty" is often treated as a binary attribute of a fact; however, our results show that in clinical streams, novelty is contextual.
Practically, this implies that CDSS interfaces must stop equating "new token generation" with "new information." The 41\% reduction in updates achieved by novelty filtering suggests that hospitals can deploy multi-agent systems without overwhelming clinicians, provided they implement middleware that actively suppresses redundancy. This shifts the integration burden from the busy clinician (who must currently filter noise) to the automated middleware, directly addressing the workflow compatibility challenge cited in recent reviews \cite{bayor2025cdss}.

\subsection{Deployment Considerations}

To move from artifact to operation, we map EXAIM into the standard four-layer implementation scheme consistent with recent DSR literature:
\begin{enumerate}
    \item \textbf{Data Layer}: Integration with FHIR-enabled EHRs to feed live patient context to upstream agents.
    \item \textbf{Model Layer}: Hosting the multi-agent reasoning engine (e.g., MedAgents) in a secure, HIPAA-compliant enclave.
    \item \textbf{Middleware Layer}: Deploying EXAIM as a low-latency interaction gateway. This layer handles the "contract" capability—ensuring that no matter how erratic the Model Layer behaves, the User Layer receives predictable JSON.
    \item \textbf{User Layer}: A React/Web-based clinician dashboard that subscribes to the EXAIM JSON stream, rendering cards that update in place.
\end{enumerate}
This layered approach isolates the clinical UI from the volatility of LLM generation, ensuring that the tool remains a "decision support" system rather than a "distraction generation" system.

\section{Conclusion}

This paper addressed the critical challenge of cognitive overload in multi-agent clinical decision support systems. We introduced EXAIM, a novel IT artifact that interposes a middleware layer to transform verbose, interleaved reasoning traces into concise, schema-constrained summaries.
Our solution is built on three validated primitives:
\begin{enumerate}
    \item \textbf{TokenGate}: which enforces syntactic coherence on streaming tokens.
    \item \textbf{BufferAgent}: which filters updates based on semantic novelty and clinical relevance.
    \item \textbf{SummarizerAgent}: which synthesizes content into SBAR/SOAP-aligned structures.
\end{enumerate}

Through systematic DSR evaluation on 40 diagnostic replay cases, we verified that EXAIM significantly outperforms structural baselines. Specifically, we demonstrated that event-driven triggering improves faithfulness by 26\% and reduced redundancy by 20\% compared to turn-based approaches. Crucially, we proved that novelty filtering is not optional but essential for preventing update fatigue.

\textbf{Limitations}: Our study is limited by its reliance on retrospective replay data, which, while deterministic, cannot capture real-time clinician behavioral adaptation. Additionally, automated faithfulness metrics, while necessary for scale, do not perfectly proxy clinical safety.

\textbf{Future Work}: Future iterations will focus on three directions: (1) deploying EXAIM in a human-in-the-loop simulation with practicing clinicians to measure decision velocity; (2) expanding the BufferAgent to detect "conflict" events where agents disagree; and (3) optimizing the middleware for edge deployment to reduce the computational cost observed in our latency metrics.

Ultimately, EXAIM contributes a reproducible architectural pattern for "High-Agency, Low-Noise" clinical AI, enabling the safe deployment of powerful multi-agent reasoners in time-critical medical environments.

\section*{Acknowledgment}

This work was supported by Dakota State University. The authors thank the MAC framework developers for making their diagnostic reasoning traces available for instrumentation.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}



