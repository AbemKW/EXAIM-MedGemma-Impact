#!/usr/bin/env python3
"""
Generate stoplists from frozen trace files ONLY.

Paper hook: "To avoid data leakage, DF was computed strictly on frozen
input traces (n=40). Concepts appearing in >=90% of traces were stoplisted,
preventing masking of variant-specific hallucinations (Section 6.1)"

CRITICAL INVARIANTS:
    1. Uses SAME extractor config as metrics computation (drift-proof)
    2. Stoplist generation with stoplists DISABLED (non-circular)
    3. Uses build_canonical_trace_text() from traces/trace_text.py
    4. Produces audit artifact for reviewer verification

Dependencies:
    - traces/trace_text.py (canonical text)
    - extraction/concept_extractor.py (CUI extraction)
    - extractor config file (extractor settings)
"""

import argparse
import csv
import hashlib
import json
import math
import sys
from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

import yaml

from ..src.traces.trace_text import build_canonical_trace_text, TraceParsingError
from .validate_traces import validate_all_traces
from ..src.config.config_loader import load_extractor_config_for_stoplist_generation


def load_phase4_extractor_config(config_path: Path) -> dict:
    """
    Load extractor config to ensure drift-proof stoplist generation.
    
    Paper hook: "Stoplist generation used identical extractor configuration
    as downstream metrics computation (Section 6.1)"
    
    DEPRECATED: Use config_loader.load_extractor_config_for_stoplist_generation()
    
    Args:
        config_path: Path to extractor.yaml config file
        
    Returns:
        Extractor configuration dict
    """
    # Use centralized loader to ensure drift-proof config
    configs_dir = config_path.parent
    return load_extractor_config_for_stoplist_generation(configs_dir)


def _file_hash(path: Path) -> str:
    """Compute SHA256 hash of file."""
    h = hashlib.sha256()
    with open(path, "rb") as f:
        h.update(f.read())
    return f"sha256:{h.hexdigest()}"


def _write_stoplist(path: Path, items: list[str]):
    """Write stoplist file with header comments."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(f"# Generated by generate_stoplists.py\n")
        f.write(f"# Generated at: {datetime.now(timezone.utc).isoformat()}\n")
        f.write(f"# Count: {len(items)}\n")
        f.write(f"#\n")
        for item in items:
            f.write(f"{item}\n")


def _write_df_report(
    path: Path,
    rows: list[dict],
    n_cases: int,
    cutoff_count: int,
    threshold: float
):
    """
    Write DF report CSV with summary stats.
    
    Paper hook: "DF report includes concept, type, df_count, df_fraction,
    status columns for full auditability (Section 6.1)"
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    
    # Sort by df_count descending, then by concept name
    rows.sort(key=lambda r: (-r["df_count"], r["concept"]))
    
    with open(path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(
            f, 
            fieldnames=["concept", "type", "df_count", "df_fraction", "status"]
        )
        writer.writeheader()
        writer.writerows(rows)
        
        # Add summary as comment rows
        f.write(f"\n# SUMMARY\n")
        f.write(f"# n_cases,{n_cases}\n")
        f.write(f"# threshold,{threshold}\n")
        f.write(f"# cutoff_count,{cutoff_count}\n")
        n_stoplisted_cuis = sum(
            1 for r in rows 
            if r['type'] == 'CUI' and r['status'] == 'STOPLISTED'
        )
        n_stoplisted_surfaces = sum(
            1 for r in rows 
            if r['type'] == 'SURFACE' and r['status'] == 'STOPLISTED'
        )
        f.write(f"# stoplisted_cuis,{n_stoplisted_cuis}\n")
        f.write(f"# stoplisted_surfaces,{n_stoplisted_surfaces}\n")
        f.write(f"# total_unique_cuis,{sum(1 for r in rows if r['type']=='CUI')}\n")
        f.write(f"# total_unique_surfaces,{sum(1 for r in rows if r['type']=='SURFACE')}\n")


def generate_stoplists_simple(
    traces_dir: Path,
    output_dir: Path,
    threshold: float = 0.90,
    verbose: bool = False
) -> dict:
    """
    Generate stoplists using simple NER-only extraction (no UMLS linking).
    
    This is a fallback when scispaCy UMLS linker is not available.
    Uses surface-form based extraction only.
    
    Paper hook: "Surface-form stoplist computed on entity mentions
    extracted via scispaCy NER (Section 6.1)"
    
    Args:
        traces_dir: Path to data/traces/
        output_dir: Path to configs/
        threshold: DF threshold (default 0.90)
        verbose: Print progress details
        
    Returns:
        Generation report dict
    """
    import spacy
    
    # Step 0: Validate traces first
    if verbose:
        print("Validating traces...")
    validation_report = validate_all_traces(traces_dir)
    
    # Step 1: Load spaCy model (NER only, no linker)
    if verbose:
        print("Loading scispaCy model...")
    try:
        nlp = spacy.load("en_core_sci_sm")
    except OSError:
        print("ERROR: scispaCy model 'en_core_sci_sm' not found.")
        print("Install with: pip install scispacy")
        print("             pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz")
        raise
    
    min_entity_len = 3
    
    # Step 2: Track per-case concept PRESENCE (not counts)
    surface_presence: dict[str, set[str]] = defaultdict(set)
    
    trace_files = sorted(traces_dir.glob("*.jsonl.gz"))
    if not trace_files:
        trace_files = sorted(traces_dir.glob("*.jsonl"))
    n_cases = len(trace_files)
    
    if verbose:
        print(f"Processing {n_cases} trace files...")
    
    for i, trace_file in enumerate(trace_files):
        case_id = trace_file.stem.replace(".jsonl", "")
        
        if verbose:
            print(f"  [{i+1}/{n_cases}] {case_id}...", end=" ")
        
        # Use canonical trace text (single source of truth)
        full_text, _ = build_canonical_trace_text(trace_file, fail_on_empty=True)
        
        # Extract surface entities
        doc = nlp(full_text)
        for ent in doc.ents:
            normalized = ent.text.lower().strip()
            if len(normalized) >= min_entity_len:
                surface_presence[normalized].add(case_id)
        
        if verbose:
            print(f"{len(doc.ents)} entities")
    
    # Step 3: Compute cutoff
    cutoff_count = math.ceil(threshold * n_cases)
    
    if verbose:
        print(f"\nDF threshold: {threshold} ({cutoff_count}/{n_cases} cases)")
    
    # Step 4: Classify and build report rows
    stop_surfaces = set()
    report_rows = []
    
    for surface, cases in surface_presence.items():
        df_count = len(cases)
        df_frac = df_count / n_cases
        status = "STOPLISTED" if df_count >= cutoff_count else "KEPT"
        if status == "STOPLISTED":
            stop_surfaces.add(surface)
        report_rows.append({
            "concept": surface,
            "type": "SURFACE",
            "df_count": df_count,
            "df_fraction": round(df_frac, 4),
            "status": status
        })
    
    # Step 5: Write outputs
    stop_entities_path = output_dir / "stop_entities.txt"
    report_path = output_dir / "stoplist_df_report.csv"
    
    _write_stoplist(stop_entities_path, sorted(stop_surfaces))
    _write_df_report(report_path, report_rows, n_cases, cutoff_count, threshold)
    
    # Create empty stop_cuis.txt (linker not used)
    stop_cuis_path = output_dir / "stop_cuis.txt"
    _write_stoplist(stop_cuis_path, [])
    
    if verbose:
        print(f"\nStoplisted {len(stop_surfaces)} surface entities")
        print(f"Output: {stop_entities_path}")
        print(f"Report: {report_path}")
    
    # Step 6: Return report with hashes
    return {
        "n_cases": n_cases,
        "threshold": threshold,
        "cutoff_count": cutoff_count,
        "n_stop_cuis": 0,  # Not computed without linker
        "n_stop_surfaces": len(stop_surfaces),
        "stop_cuis_hash": _file_hash(stop_cuis_path),
        "stop_entities_hash": _file_hash(stop_entities_path),
        "stoplist_df_report_hash": _file_hash(report_path),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "extraction_mode": "surface_only",
        "validation_report": validation_report
    }


def generate_stoplists_with_linking(
    traces_dir: Path,
    output_dir: Path,
    phase4_config_path: Path,
    threshold: float = 0.90,
    verbose: bool = False
) -> dict:
    """
    Generate stoplists with full UMLS CUI linking.
    
    Paper hook: "CUI-based stoplist computed using scispaCy UMLS linker
    with identical configuration as downstream metrics (Section 6.1)"
    
    Args:
        traces_dir: Path to data/traces/
        output_dir: Path to configs/
        phase4_config_path: Path to extractor config
        threshold: DF threshold (default 0.90)
        verbose: Print progress details
        
    Returns:
        Generation report dict
    """
    # Import here to allow fallback mode without these deps
    import spacy
    try:
        from scispacy.linking import EntityLinker
        HAS_LINKER = True
    except ImportError:
        HAS_LINKER = False
        print("WARNING: scispaCy EntityLinker not available, using surface-only mode")
        return generate_stoplists_simple(traces_dir, output_dir, threshold, verbose)
    
    # Step 0: Validate traces first
    if verbose:
        print("Validating traces...")
    validation_report = validate_all_traces(traces_dir)
    
    # Step 1: Load extractor config (drift-proof)
    if verbose:
        print("Loading extractor config...")
    base_config = load_phase4_extractor_config(phase4_config_path)
    
    # Step 2: Override ONLY stoplist fields (non-circular)
    # This ensures we don't filter concepts using the stoplists we're generating
    extractor_config = {
        **base_config,
        "stop_entities_file": None,  # DISABLED for generation
        "stop_cuis_file": None       # DISABLED for generation
    }
    
    # Step 3: Initialize NLP pipeline
    if verbose:
        print("Loading scispaCy model with UMLS linker...")
    
    nlp = spacy.load(extractor_config.get("scispacy_model", "en_core_sci_sm"))
    
    # Add UMLS linker with config from extractor settings
    linker_config = {
        "resolve_abbreviations": extractor_config.get("linker_resolve_abbreviations", True),
        "linker_name": extractor_config.get("linker_name", "umls"),
        "max_entities_per_mention": extractor_config.get("linker_max_entities_per_mention", 10),
        "threshold": extractor_config.get("linker_threshold", 0.7)
    }
    
    try:
        nlp.add_pipe("scispacy_linker", config=linker_config)
    except Exception as e:
        print(f"WARNING: Failed to add UMLS linker: {e}")
        print("Falling back to surface-only mode")
        return generate_stoplists_simple(traces_dir, output_dir, threshold, verbose)
    
    min_entity_len = extractor_config.get("min_entity_len", 3)
    cui_score_threshold = extractor_config.get("cui_score_threshold", 0.7)
    max_k = extractor_config.get("max_k", 10)
    
    # Step 4: Track per-case concept PRESENCE
    cui_presence: dict[str, set[str]] = defaultdict(set)
    surface_presence: dict[str, set[str]] = defaultdict(set)
    
    trace_files = sorted(traces_dir.glob("*.jsonl.gz"))
    if not trace_files:
        trace_files = sorted(traces_dir.glob("*.jsonl"))
    n_cases = len(trace_files)
    
    if verbose:
        print(f"Processing {n_cases} trace files...")
    
    for i, trace_file in enumerate(trace_files):
        case_id = trace_file.stem.replace(".jsonl", "")
        
        if verbose:
            print(f"  [{i+1}/{n_cases}] {case_id}...", end=" ")
        
        # Use canonical trace text (single source of truth)
        full_text, _ = build_canonical_trace_text(trace_file, fail_on_empty=True)
        
        # Extract concepts
        doc = nlp(full_text)
        entity_count = 0
        cui_count = 0
        
        for ent in doc.ents:
            # Surface extraction
            normalized_surface = ent.text.lower().strip()
            if len(normalized_surface) >= min_entity_len:
                surface_presence[normalized_surface].add(case_id)
                entity_count += 1
            
            # CUI extraction with filter→sort→topK ordering
            if hasattr(ent._, 'kb_ents') and ent._.kb_ents:
                candidates = list(ent._.kb_ents)
                
                # Filter by score threshold
                valid_candidates = [
                    (cui, score) for cui, score in candidates
                    if score >= cui_score_threshold
                ]
                
                # Sort descending by score
                valid_candidates.sort(key=lambda x: x[1], reverse=True)
                
                # Take top K
                top_k = valid_candidates[:max_k]
                
                for cui, score in top_k:
                    cui_normalized = cui.upper()  # Canonical uppercase
                    cui_presence[cui_normalized].add(case_id)
                    cui_count += 1
        
        if verbose:
            print(f"{entity_count} entities, {cui_count} CUIs")
    
    # Step 5: Compute cutoff
    cutoff_count = math.ceil(threshold * n_cases)
    
    if verbose:
        print(f"\nDF threshold: {threshold} ({cutoff_count}/{n_cases} cases)")
    
    # Step 6: Classify and build report rows
    stop_cuis = set()
    stop_surfaces = set()
    report_rows = []
    
    for cui, cases in cui_presence.items():
        df_count = len(cases)
        df_frac = df_count / n_cases
        status = "STOPLISTED" if df_count >= cutoff_count else "KEPT"
        if status == "STOPLISTED":
            stop_cuis.add(cui)
        report_rows.append({
            "concept": cui,
            "type": "CUI",
            "df_count": df_count,
            "df_fraction": round(df_frac, 4),
            "status": status
        })
    
    for surface, cases in surface_presence.items():
        df_count = len(cases)
        df_frac = df_count / n_cases
        status = "STOPLISTED" if df_count >= cutoff_count else "KEPT"
        if status == "STOPLISTED":
            stop_surfaces.add(surface)
        report_rows.append({
            "concept": surface,
            "type": "SURFACE",
            "df_count": df_count,
            "df_fraction": round(df_frac, 4),
            "status": status
        })
    
    # Step 7: Write outputs
    stop_cuis_path = output_dir / "stop_cuis.txt"
    stop_entities_path = output_dir / "stop_entities.txt"
    report_path = output_dir / "stoplist_df_report.csv"
    
    _write_stoplist(stop_cuis_path, sorted(stop_cuis))
    _write_stoplist(stop_entities_path, sorted(stop_surfaces))
    _write_df_report(report_path, report_rows, n_cases, cutoff_count, threshold)
    
    if verbose:
        print(f"\nStoplisted {len(stop_cuis)} CUIs, {len(stop_surfaces)} surfaces")
        print(f"Output CUIs: {stop_cuis_path}")
        print(f"Output surfaces: {stop_entities_path}")
        print(f"Report: {report_path}")
    
    # Step 8: Return report with hashes
    return {
        "n_cases": n_cases,
        "threshold": threshold,
        "cutoff_count": cutoff_count,
        "n_stop_cuis": len(stop_cuis),
        "n_stop_surfaces": len(stop_surfaces),
        "stop_cuis_hash": _file_hash(stop_cuis_path),
        "stop_entities_hash": _file_hash(stop_entities_path),
        "stoplist_df_report_hash": _file_hash(report_path),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "phase4_config_used": str(phase4_config_path),
        "extraction_mode": "cui_linking",
        "linker_config": linker_config,
        "validation_report": validation_report
    }


def generate_stoplists(
    traces_dir: Path,
    output_dir: Path,
    phase4_config_path: Optional[Path] = None,
    threshold: float = 0.90,
    verbose: bool = False
) -> dict:
    """
    Generate stoplists with extractor config but stoplists DISABLED.
    
    Paper hook: "Stoplist generation uses identical extractor configuration
    as metrics computation, ensuring drift-proof evaluation (Section 6.1)"
    
    Args:
        traces_dir: Path to data/traces/
        output_dir: Path to configs/
        phase4_config_path: Path to extractor config (optional)
        threshold: DF threshold (default 0.90)
        verbose: Print progress details
        
    Returns:
        Generation report dict
    """
    if phase4_config_path and phase4_config_path.exists():
        return generate_stoplists_with_linking(
            traces_dir, output_dir, phase4_config_path, threshold, verbose
        )
    else:
        if verbose:
            print("No extractor config provided, using surface-only mode")
        return generate_stoplists_simple(
            traces_dir, output_dir, threshold, verbose
        )


def main():
    parser = argparse.ArgumentParser(
        description="Generate stoplists from frozen traces"
    )
    parser.add_argument(
        "--traces",
        type=Path,
        default=Path("data/traces"),
        help="Input traces directory (default: data/traces)"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("configs"),
        help="Output configs directory (default: configs)"
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=Path("configs/extractor.yaml"),
        help="Extractor config (default: configs/extractor.yaml)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.90,
        help="DF threshold for stoplisting (default: 0.90)"
    )
    parser.add_argument(
        "--report",
        type=Path,
        help="Output generation report JSON (optional)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Print progress details"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("EXAID Stoplist Generation")
    print("=" * 60)
    print()
    print(f"Traces directory: {args.traces}")
    print(f"Output directory: {args.output}")
    print(f"Extractor config: {args.config}")
    print(f"DF threshold: {args.threshold}")
    print()
    
    try:
        report = generate_stoplists(
            args.traces,
            args.output,
            args.config if args.config.exists() else None,
            args.threshold,
            args.verbose
        )
    except TraceParsingError as e:
        print(f"STOPLIST GENERATION FAILED: {e}")
        return 1
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    print()
    print("=" * 60)
    print("STOPLIST GENERATION COMPLETE")
    print("=" * 60)
    print()
    print(f"  Cases processed: {report['n_cases']}")
    print(f"  DF threshold: {report['threshold']}")
    print(f"  Cutoff count: {report['cutoff_count']}")
    print(f"  Stoplisted CUIs: {report['n_stop_cuis']}")
    print(f"  Stoplisted surfaces: {report['n_stop_surfaces']}")
    print(f"  Extraction mode: {report.get('extraction_mode', 'unknown')}")
    print()
    print("Provenance hashes:")
    print(f"  stop_cuis: {report['stop_cuis_hash']}")
    print(f"  stop_entities: {report['stop_entities_hash']}")
    print(f"  report: {report['stoplist_df_report_hash']}")
    
    if args.report:
        with open(args.report, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2)
        print(f"\nFull report saved to: {args.report}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
